// GENERATED BY VOLTRON.PY SCRIPT
/*
METAMALLOC VERSION 1.0.0

MIT License

Copyright (c) 2024 Akin Ocal

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
*/
#ifndef __METAMALLOC__
#define __METAMALLOC__

// STD C
#include <atomic>
#include <cstddef>
#include <cstdint>
#include <cstdarg>
#include <cmath>
#include <cassert>
#include <cstring>
#include <cstdlib>
// STD
#include <type_traits>
#include <array>
#include <string_view>
#include <new>
// CPU INTRINSICS
#include <immintrin.h>
#if defined(_MSC_VER)
#include <intrin.h>
#elif defined(__GNUC__)
#include <emmintrin.h>
#endif
// LINUX
#ifdef __linux__
#include <unistd.h>
#include <sys/mman.h>
#include <pthread.h>
#include <sched.h>
#include <sys/types.h>
#include <fcntl.h>
#ifdef ENABLE_NUMA
#include <numa.h>
#include <numaif.h>
#endif
#endif
// STATS
#ifdef ENABLE_STATS
#include <string>
#include <fstream>
#include <sstream>
#include <iomanip>
#endif
#if defined(ENABLE_PERF_TRACES) || defined(ENABLE_REPORT_LEAKS)
#include <cstdio>
#endif
// WINDOWS
#ifdef _WIN32
#include <windows.h>
#include <fibersapi.h>
#endif
// UNIT TESTS
#ifdef UNIT_TEST
#include <string>
#include <cstdio>
#endif

namespace metamalloc
{
#ifndef _CHECKS_H_
#define _CHECKS_H_

//////////////////////////////////////////////////////////////////////
// COMPILER CHECK
#if (! defined(_MSC_VER)) && (! defined(__GNUC__))
#error "This library is supported for only GCC and MSVC compilers"
#endif

//////////////////////////////////////////////////////////////////////
// C++ VERSION CHECK
#if defined(_MSC_VER)
#if _MSVC_LANG < 201703L
#error "This library requires to be compiled with C++17"
#endif

#elif defined(__GNUC__)
#if __cplusplus < 201703L
#error "This library requires to be compiled with C++17"
#endif

#endif

#endif
#ifndef _ARCHITECTURE_CHECK_H_
#define _ARCHITECTURE_CHECK_H_

//////////////////////////////////////////////////////////////////////
// ARCHITECTURE CHECK

#if defined(_MSC_VER)
#if (! defined(_M_X64))
#error "This library is supported for only x86-x64 architectures"
#endif

#elif defined(__GNUC__)
#if (! defined(__x86_64__)) && (! defined(__x86_64))
#error "This library is supported for only x86-x64 architectures"
#endif

#endif

#endif
#ifndef _OS_CHECK_
#define _OS_CHECK_

//////////////////////////////////////////////////////////////////////
// OPERATING SYSTEM CHECK

#if (! defined(__linux__)) && (! defined(_WIN32) )
#error "This library is supported for Linux and Windows systems"
#endif

#endif
#ifndef _BUILTIN_FUNCTIONS_H_
#define _BUILTIN_FUNCTIONS_H_

//////////////////////////////////////////////////////////////////////
// Count trailing zeroes
#if defined(__GNUC__)
#define builtin_ctzl(n)     __builtin_ctzl(n)
#elif defined(_MSC_VER)
#if defined(_WIN64)    // Implementation is for 64-bit only.
inline unsigned int builtin_ctzl(unsigned long long value)
{
    unsigned long trailing_zero = 0;

    if (_BitScanForward64(&trailing_zero, static_cast<unsigned __int64>(value)))
    {
        return static_cast<unsigned int>(trailing_zero);
    }
    else
    {
        return 64;    // Sizeof unsigned long long.
    }
}
#else
#error "This code is intended for 64-bit Windows platforms only."
#endif

#endif

//////////////////////////////////////////////////////////////////////
// Count leading zeroes
#if defined(__GNUC__)
#define builtin_clzl(n)     __builtin_clzl(n)
#elif defined(_MSC_VER)
#if defined(_WIN64)    // Implementation is for 64-bit only.
inline int builtin_clzl(unsigned long value)
{
    unsigned long index = 0;
    return _BitScanReverse64(&index, static_cast<unsigned __int64>(value)) ? static_cast<int>(63 - index) : 64;
}
#else
#error "This code is intended for 64-bit Windows platforms only."
#endif

#endif

//////////////////////////////////////////////////////////////////////
// Compare and swap, standard C++ provides them however it requires non-POD std::atomic usage
// They are needed when we want to embed spinlocks in "packed" data structures which need all members to be POD such as headers
#if defined(__GNUC__)
#define builtin_cas(pointer, old_value, new_value) __sync_val_compare_and_swap(pointer, old_value, new_value)
#elif defined(_MSC_VER)
#define builtin_cas(pointer, old_value, new_value) _InterlockedCompareExchange(reinterpret_cast<long*>(pointer), new_value, old_value)
#endif

//////////////////////////////////////////////////////////////////////
// memcpy
#if defined(__GNUC__)
#define builtin_memcpy(destination, source, size)     __builtin_memcpy(destination, source, size)
#elif defined(_MSC_VER)
#define builtin_memcpy(destination, source, size)     std::memcpy(destination, source, size)
#endif

//////////////////////////////////////////////////////////////////////
// memset
#if defined(__GNUC__)
#define builtin_memset(destination, character, count)  __builtin_memset(destination, character, count)
#elif defined(_MSC_VER)
#define builtin_memset(destination, character, count)  std::memset(destination, character, count)
#endif

//////////////////////////////////////////////////////////////////////
// aligned_alloc , It exists because MSVC does not provide std::aligned_alloc
#if defined(__GNUC__)
#define builtin_aligned_alloc(size, alignment)  std::aligned_alloc(alignment, size)
#define builtin_aligned_free(ptr)                 std::free(ptr)
#elif defined(_MSC_VER)
#define builtin_aligned_alloc(size, alignment)  _aligned_malloc(size, alignment)
#define builtin_aligned_free(ptr)                 _aligned_free(ptr)
#endif

#endif
#ifndef _HINTS_BRANCH_PREDICTOR_
#define _HINTS_BRANCH_PREDICTOR_

//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// LIKELY
#if defined(_MSC_VER)
//No implementation provided for MSVC for pre C++20 :
//https://social.msdn.microsoft.com/Forums/vstudio/en-US/2dbdca4d-c0c0-40a3-993b-dc78817be26e/branch-hints?forum=vclanguage
#define likely(x) x
#elif defined(__GNUC__)
#define likely(x)      __builtin_expect(!!(x), 1)
#endif

//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// UNLIKELY
#if defined(_MSC_VER)
//No implementation provided for MSVC for pre C++20 :
//https://social.msdn.microsoft.com/Forums/vstudio/en-US/2dbdca4d-c0c0-40a3-993b-dc78817be26e/branch-hints?forum=vclanguage
#define unlikely(x) x
#elif defined(__GNUC__)
#define unlikely(x)    __builtin_expect(!!(x), 0)
#endif

//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// VERY LIKELY
#if defined(_MSC_VER)
//No implementation provided for MSVC in any version :
//https://social.msdn.microsoft.com/Forums/vstudio/en-US/2dbdca4d-c0c0-40a3-993b-dc78817be26e/branch-hints?forum=vclanguage
#define very_likely(x) x
#elif defined(__GNUC__)
#define very_likely(x) __builtin_expect_with_probability(!!(x),1,0.99)
#endif

#endif
#ifndef _HINTS_HOT_CODE_
#define _HINTS_HOT_CODE_

//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// FORCE_INLINE
#if defined(_MSC_VER)
#define FORCE_INLINE __forceinline
#elif defined(__GNUC__)
#define FORCE_INLINE __attribute__((always_inline))
#endif

//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// HOT
#if defined(_MSC_VER)
//No implementation provided for MSVC :
#define HOT
#elif defined(__GNUC__)
#define HOT __attribute__((hot))
#endif

//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// ALIGN_DATA , some GCC versions gives warnings about standard C++ 'alignas' when applied to data
#ifdef __GNUC__
#define ALIGN_DATA( _alignment_ ) __attribute__((aligned( (_alignment_) )))
#elif _MSC_VER
#define ALIGN_DATA( _alignment_ ) alignas( _alignment_ )
#endif

//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// ALIGN_CODE, using alignas(64) or __attribute__(aligned(alignment)) for a function will work in GCC but MSVC won't compile
#ifdef __GNUC__
#define ALIGN_CODE( _alignment_ ) __attribute__((aligned( (_alignment_) )))
#elif _MSC_VER
//No implementation provided for MSVC :
#define ALIGN_CODE( _alignment_ )
#endif

//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// FORCE_LOOP_VECTORISATION
/*
    Usage :

        FORCE_LOOP_VECTORISATION
        for (int j = 0; j < M; j++)
        {
            // loop body
        }

    _Pragma ( since C++11 ) allows to use pragma directives in macros.
    In MSVC, it is not supported but instead MSVC provides __pragma : https://learn.microsoft.com/en-us/cpp/preprocessor/pragma-directives-and-the-pragma-keyword?view=msvc-170

    In MSVC, if you pass "-openmp:experimental" option to the compiler , it will inform you if vectorization failed :

                 info C5002: Omp simd loop not vectorized due to reason '1303' -> means that there was too few iterations
*/
#ifdef _MSC_VER
#define FORCE_LOOP_VECTORISATION __pragma(omp simd)
/*
    MSVC also supports #pragma ivdep
    But they recommend pragma omp simd :
    https://learn.microsoft.com/en-us/cpp/parallel/openmp/openmp-simd?view=msvc-170
*/
#elif defined(__GNUC__)
#define FORCE_LOOP_VECTORISATION _Pragma("GCC ivdep")
#endif

#endif
#ifndef _PACKED_H_
#define _PACKED_H_

//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// PACKED

// Compilers may add additional padding zeroes for alignment
// Though those additions may increase the size of your structs/classes
// The ideal way is manually aligning data structures and minimising the memory footprint
// Compilers won`t add additional padding zeroes for "packed" data structures

#ifdef __GNUC__
#define PACKED( __Declaration__ ) __Declaration__ __attribute__((__packed__))
#elif _MSC_VER
#define PACKED( __Declaration__ ) __pragma( pack(push, 1) ) __Declaration__ __pragma( pack(pop))
#endif

#endif
#ifndef _UNUSED_H_
#define _UNUSED_H_

//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// UNUSED
//To avoid unused variable warnings
#if defined(__GNUC__)
#define UNUSED(x) (void)(x)
#elif defined(_MSC_VER)
#define UNUSED(x) __pragma(warning(suppress:4100)) x
#endif

#endif
#ifndef _ALIGNMENT_CONSTANTS_
#define _ALIGNMENT_CONSTANTS_

namespace AlignmentConstants
{
    // All constants are in bytes
    constexpr std::size_t CACHE_LINE_SIZE = 64;
    // SIMD REGISTER WIDTHS
    constexpr std::size_t SIMD_SSE42_WIDTH = 16;
    constexpr std::size_t SIMD_AVX_WIDTH = 32;
    constexpr std::size_t SIMD_AVX2_WIDTH = 32;
    constexpr std::size_t SIMD_AVX512_WIDTH = 64;
    constexpr std::size_t MINIMUM_VECTORISATION_WIDTH = SIMD_SSE42_WIDTH;
    constexpr std::size_t LARGEST_VECTORISATION_WIDTH = SIMD_AVX512_WIDTH; // AVX10 not available yet
    // VIRTUAL MEMORY PAGE SIZES ARE HANDLED IN os/virtual_memory.h
}

#endif
#ifndef _PAUSE_H_
#define _PAUSE_H_

/*
    Intel initially advised using _mm_pause in spin-wait loops in case of hyperthreading
    Before Skylake it was about 10 cycles, but with Skylake it becomes 140 cycles and that applies to successor architectures
    -> Intel opt manual 2.5.4 "Pause Latency in Skylake Client Microarchitecture"

    Later _tpause  / _umonitor / _umwait instructions were introduced however not using them for the time being as they are not widespread yet

    Pause implementation is instead using nop
*/

inline void pause(uint16_t repeat_count=100)
{
    #if defined(__GNUC__)
    // rep is for repeating by the no provided in 16 bit cx register
    __asm__ __volatile__("mov %0, %%cx\n\trep; nop" : : "r" (repeat_count) : "cx");
    #elif defined(_WIN32)
    for (uint16_t i = 0; i < repeat_count; ++i)
    {
        _mm_lfence();
        __nop();
        _mm_lfence();
    }
    #endif

}

#endif
// 64BIT ONLY LIVE FUNCTION CODE REPLACEMENT VIA MEMORY MANIPULATION FOR ONLY WINDOWS AS THERE IS NO NEED ON LINUX THANKS TO LD_PRELOAD
#ifndef __TRAMPOLINE_H__
#define __TRAMPOLINE_H__

#ifdef _WIN32

class ScopedReadWriteAccess
{
    public:

        ScopedReadWriteAccess(void* address)
        {
            VirtualQuery(address, &m_mbi_thunk, sizeof(MEMORY_BASIC_INFORMATION));
            VirtualProtect(m_mbi_thunk.BaseAddress, m_mbi_thunk.RegionSize, PAGE_EXECUTE_READWRITE, &m_mbi_thunk.Protect);
        };

        ~ScopedReadWriteAccess()
        {
            VirtualProtect(m_mbi_thunk.BaseAddress, m_mbi_thunk.RegionSize, m_mbi_thunk.Protect, &m_mbi_thunk.Protect);
        }
    private:
        MEMORY_BASIC_INFORMATION m_mbi_thunk;
};

class Trampoline // SUPPORTS ONLY 64 BIT
{
    public :

        static inline constexpr std::size_t INSTRUCTION_SIZE = 14; // 16bits opcode + 32 bits offset + 64 bits address
        using Bytes = char[INSTRUCTION_SIZE];

        static bool install(void* original_function_address, void* replacement_function_address, Bytes original_bytes)
        {
            if (is_address_64_bit(original_function_address) == false || is_address_64_bit(replacement_function_address) == false) return false;

            ScopedReadWriteAccess scoped_read_write(original_function_address);

            builtin_memcpy(original_bytes, original_function_address, INSTRUCTION_SIZE);
            auto* target = reinterpret_cast<char*>(original_function_address);

            // FIRST 16 BITS - FARJMP opcode 0x25ff
            uint16_t farjmp_opcode = 0x25ff;
            builtin_memcpy(target, &farjmp_opcode, 2);

            // FOLLOWING 32 BITS, OFFSET
            uint32_t offset = 0;
            builtin_memcpy(target+2, &offset, 4);

            // FOLLOWING 64 BITS REPLACEMENT ADDRESS
            builtin_memcpy(target+6, &replacement_function_address, 8);

            return true;
        }

        static void uninstall(void* original_function_address, Bytes original_bytes)
        {
            ScopedReadWriteAccess scoped_read_write(original_function_address);
            builtin_memcpy(original_function_address, original_bytes, INSTRUCTION_SIZE);
        }

    private:

        static bool is_address_64_bit(void* address)
        {
            MEMORY_BASIC_INFORMATION mbi;
            VirtualQuery(address, &mbi, sizeof(MEMORY_BASIC_INFORMATION));
            return mbi.BaseAddress < (void*)0x8000000000000000;
        }
};

#endif

#endif
/*
    - To work with huge pages , you may need to configure your system :

        - Linux : /proc/meminfo should have non-zero "Hugepagesize" & "HugePages_Total/HugePages_Free" attributes
                  ( If HugePages_Total or HugePages_Free  is 0
                  then run "echo 20 | sudo tee /proc/sys/vm/nr_hugepages" to reserve huge pages
                  Reference : https://www.kernel.org/doc/Documentation/vm/hugetlbpage.txt )

                  ( If THP is enabled , we will use madvise. Otherwise we will use HUGE_TLB flag for mmap.
                  To check if THP enabled : sudo cat /sys/kernel/mm/transparent_hugepage/enabled
                  To disable THP : echo never | sudo tee /sys/kernel/mm/transparent_hugepage/enable
                  )

        - Windows : SeLockMemoryPrivilege is required.
                    It can be acquired using gpedit.msc :
                    Local Computer Policy -> Computer Configuration -> Windows Settings -> Security Settings -> Local Policies -> User Rights Managements -> Lock pages in memory

    - There is NUMA functionality however it is only experimental and not compiled by default. To try NUMA :

            You need : #define ENABLE_NUMA

            Also if on Linux , you need libnuma ( For ex : on Ubuntu -> sudo apt install libnuma-dev ) and -lnuma for GCC
*/

#ifndef __VIRTUAL_MEMORY_H__
#define __VIRTUAL_MEMORY_H__

#ifdef _WIN32
#pragma warning(disable:6250)
#endif

class VirtualMemory
{
    public:

        constexpr static std::size_t NO_NUMA = -1;

        #ifdef __linux__
        constexpr static std::size_t PAGE_ALLOCATION_GRANULARITY = 4096;    // In bytes
        #elif _WIN32
        constexpr static std::size_t PAGE_ALLOCATION_GRANULARITY = 65536;    // In bytes , https://devblogs.microsoft.com/oldnewthing/20031008-00/?p=42223
        #endif

        static std::size_t get_page_size()
        {
            std::size_t ret{ 0 };

            #ifdef __linux__
            ret = static_cast<std::size_t>(sysconf(_SC_PAGESIZE));        // TYPICALLY 4096, 2^ 12
            #elif _WIN32
            // https://learn.microsoft.com/en-gb/windows/win32/api/sysinfoapi/ns-sysinfoapi-system_info
            SYSTEM_INFO system_info;
            GetSystemInfo(&system_info);
            ret = system_info.dwPageSize; // TYPICALLY 4096, 2^ 12
            #endif

            return ret;
        }

        static bool is_huge_page_available()
        {
            bool ret{ false };
            #ifdef __linux__
            if (get_huge_page_size() <= 0)
            {
                ret = false;
            }
            else
            {
                if ( get_huge_page_total_count() > 0 )
                {
                    ret = true;
                }
            }
            #elif _WIN32
            auto huge_page_size = get_huge_page_size();

            if (huge_page_size)
            {
                HANDLE token = 0;
                OpenProcessToken(GetCurrentProcess(), TOKEN_ADJUST_PRIVILEGES | TOKEN_QUERY, &token);

                if (token)
                {
                    LUID luid;

                    if (LookupPrivilegeValue(0, SE_LOCK_MEMORY_NAME, &luid))
                    {
                        TOKEN_PRIVILEGES token_privileges;
                        memset(&token_privileges, 0, sizeof(token_privileges));
                        token_privileges.PrivilegeCount = 1;
                        token_privileges.Privileges[0].Luid = luid;
                        token_privileges.Privileges[0].Attributes = SE_PRIVILEGE_ENABLED;

                        if (AdjustTokenPrivileges(token, FALSE, &token_privileges, 0, 0, 0))
                        {
                            auto last_error = GetLastError();

                            if (last_error  == ERROR_SUCCESS)
                            {
                                ret = true;
                            }
                        }
                    }
                }
            }

            #endif

            return ret;
        }

        static std::size_t get_huge_page_size()
        {
            std::size_t ret{ 0 };
            #ifdef __linux__
            ret = get_proc_mem_info("Hugepagesize", 13) * 1024; // It is in KBs
            #elif _WIN32
            ret = static_cast<std::size_t>(GetLargePageMinimum());
            #endif

            return ret;
        }

        #ifdef __linux__
        // Equivalent of /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages
        static std::size_t get_huge_page_total_count()
        {
            auto ret = get_proc_mem_info("HugePages_Total", 16);
            if(ret == 0 )
            {
                ret = get_proc_mem_info("HugePages_Free", 15);
            }
            return ret;
        }

        static std::size_t get_proc_mem_info(const char* attribute, std::size_t attribute_len)
        {
            // Using syscalls to avoid memory allocations
            std::size_t ret = 0;
            const char* mem_info_file = "/proc/meminfo";

            int fd = open(mem_info_file, O_RDONLY);
            if (fd < 0) {
            return ret;
            }

            char buffer[256];
            size_t read_bytes;

            while ((read_bytes = read(fd, buffer, sizeof(buffer))) > 0)
            {
            char* pos = strstr(buffer, attribute);

            if (pos != nullptr)
            {
                ret = std::strtoul(pos + attribute_len, nullptr, 10);
                break;
            }
            }

            close(fd);

            return ret;
        }

        // THP stands for "transparent huge page". A Linux mechanism
        // It affects how we handle allocation of huge pages on Linux
        static bool is_thp_enabled()
        {
            // Using syscalls to avoid memory allocation
            const char* thp_enabled_file = "/sys/kernel/mm/transparent_hugepage/enabled";

            // Use the access system call to check if the file exists
            if (access(thp_enabled_file, F_OK) == 0)
            {
                return true;
            }

            return false;
        }
        #endif

        #ifdef ENABLE_NUMA
        static std::size_t get_numa_node_count()
        {
            std::size_t ret{ 0 };

            #ifdef __linux__
            // Requires -lnuma
            ret = static_cast<std::size_t>(numa_num_configured_nodes());
            #elif _WIN32
            // GetNumaHighestNodeNumber is not guaranteed to be equal to NUMA node count so we need to iterate backwards
            ULONG current_numa_node = 0;
            GetNumaHighestNodeNumber(&current_numa_node);

            while (current_numa_node > 0)
            {
                GROUP_AFFINITY affinity;
                if ((GetNumaNodeProcessorMaskEx)(static_cast<USHORT>(current_numa_node), &affinity))
                {
                    //If the specified node has no processors configured, the Mask member is zero
                    if (affinity.Mask != 0)
                    {
                        ret++;
                    }
                }
                // max node was invalid or had no processor assigned, try again
                current_numa_node--;
            }
            #endif

            return ret;
        }

        static std::size_t get_numa_node_of_caller()
        {
            std::size_t numa_node = -1;
            #ifdef __linux__
            // Requires -lnuma
            numa_node = static_cast<std::size_t>(numa_node_of_cpu(sched_getcpu()));
            #elif _WIN32
            USHORT node_number{ 0 };
            PROCESSOR_NUMBER processor_number{ 0 };
            GetCurrentProcessorNumberEx(&processor_number);
            if (GetNumaProcessorNodeEx(&processor_number, &node_number))
            {
                numa_node = static_cast<std::size_t>(node_number);
            }
            #endif

            return numa_node;
        }
        #endif

        // Note about alignments : Windows always returns page ( typically 4KB ) or huge page ( typially 2MB ) aligned addresses
        //                           On Linux , page sized ( again 4KB) allocations are aligned to 4KB, but the same does not apply to huge page allocations : They are aligned to 4KB but never to 2MB
        //                           Therefore in case of huge page use, there is no guarantee that the allocated address will be huge-page-aligned , so alignment requirements have to be handled by the caller
        //
        // Note about huge page failures : If huge page allocation fails, for the time being not doing a fallback for a subsequent non huge page allocation
        //                                    So library users have to check return values
        //
        template <bool use_hugepage, std::size_t numa_node=NO_NUMA, bool zero_buffer=false>
        static void* allocate(std::size_t size, void* hint_address = nullptr)
        {
            void* ret = nullptr;
            #ifdef __linux__
            static bool thp_enabled = is_thp_enabled();
            // MAP_ANONYMOUS rather than going thru a file (memory mapped file)
            // MAP_PRIVATE rather than shared memory
            int flags = MAP_PRIVATE | MAP_ANONYMOUS;

            #ifndef ENABLE_NUMA
            // MAP_POPULATE forces system to access the  just-allocated memory. That helps by creating TLB entries
            flags |= MAP_POPULATE;
            #endif

            if constexpr (use_hugepage)
            {
                if (!thp_enabled)
                {
                    flags |= MAP_HUGETLB;
                }
            }

            ret = mmap(hint_address, size, PROT_READ | PROT_WRITE, flags, -1, 0);

            if (ret == nullptr)
            {
                return ret;
            }

            if constexpr (use_hugepage)
            {
                if (thp_enabled)
                {
                    madvise(ret, size, MADV_HUGEPAGE);
                }
            }

            #ifdef ENABLE_NUMA
            auto numa_node_count = get_numa_node_count();

            if (numa_node_count > 0 && numa_node != static_cast<std::size_t>(-1))
            {
                unsigned long nodemask = 1UL << numa_node;
                int result = mbind(ret, size, MPOL_BIND, &nodemask, sizeof(nodemask), MPOL_MF_MOVE);

                if (result != 0)
                {
                    munmap(ret, size);
                    ret = nullptr;
                }
            }
            #endif

            #elif _WIN32
            int flags = MEM_RESERVE | MEM_COMMIT;

            if constexpr (use_hugepage)
            {
                flags |= MEM_LARGE_PAGES;
            }

            #ifndef ENABLE_NUMA
            ret = VirtualAlloc(hint_address, size, flags, PAGE_READWRITE);
            #else
            auto numa_node_count = get_numa_node_count();

            if (numa_node_count > 0 && numa_node != static_cast<std::size_t>(-1))
            {
                ret = VirtualAllocExNuma(GetCurrentProcess(), hint_address, size, flags, PAGE_READWRITE, static_cast<DWORD>(numa_node));
            }
            else
            {
                ret = VirtualAlloc(hint_address, size, flags, PAGE_READWRITE);
            }
            #endif

            #endif

            if constexpr (zero_buffer)
            {
                builtin_memset(ret, 0, size);
            }

            return ret;
        }

        static bool deallocate(void* address, std::size_t size)
        {
            bool ret{ false };
            #ifdef __linux__
            ret = munmap(address, size) == 0 ? true : false;
            #elif _WIN32
            ret = VirtualFree(address, size, MEM_RELEASE) ? true : false;
            #endif

            return ret;
        }

        static bool lock_all_pages()
        {
            bool ret{ false };
            #ifdef __linux__
            ret = mlockall(MCL_CURRENT | MCL_FUTURE) == 0 ? true : false;
            #elif _WIN32
            ret = false; // No equivalent on Windows , you have to use VirtualLock per individual page
            #endif

            return ret;
        }

        // To prevent the system from swapping the pages out to the paging file
        static bool lock(void* address, std::size_t size)
        {
            bool ret{ false };
            #ifdef __linux__
            ret = mlock(address, size) == 0 ? true : false;
            #elif _WIN32
            ret = VirtualLock(address, size) ? true : false;
            #endif

            return ret;
        }

        static bool unlock(void* address, std::size_t size)
        {
            bool ret{ false };
            #ifdef __linux__
            ret = munlock(address, size) == 0 ? true : false;
            #elif _WIN32
            ret = VirtualUnlock(address, size) ? true : false;
            #endif

            return ret;
        }

    private :
};

#endif

/*  
    Standard C++ thread_local keyword does not allow you to specify thread specific destructors
    and also can't be applied to class members
*/
#ifndef _THREAD_LOCAL_STORAGE_
#define _THREAD_LOCAL_STORAGE_

class ThreadLocalStorage
{
    public:

        static ThreadLocalStorage& get_instance()
        {
            static ThreadLocalStorage instance;
            return instance;
        }

        // Call it only once for a process
        bool create(void(*thread_destructor)(void*) = nullptr)
        {
            #if __linux__
            return pthread_key_create(&m_tls_index, thread_destructor) == 0;
            #elif _WIN32
            // Using FLSs rather TLS as it is identical + TLSAlloc doesn't support dtor
            m_tls_index = FlsAlloc(thread_destructor);
            return m_tls_index == FLS_OUT_OF_INDEXES ? false : true;
            #endif

        }

        // Same as create
        void destroy()
        {
            if (m_tls_index)
            {
                #if __linux__
                pthread_key_delete(m_tls_index);
                #elif _WIN32
                FlsFree(m_tls_index);
                #endif

                m_tls_index = 0;
            }
        }

        // GUARANTEED TO BE THREAD-SAFE/LOCAL
        void* get()
        {
            #if __linux__
            return pthread_getspecific(m_tls_index);
            #elif _WIN32
            return FlsGetValue(m_tls_index);
            #endif

        }

        void set(void* data_address)
        {
            #if __linux__
            pthread_setspecific(m_tls_index, data_address);
            #elif _WIN32
            FlsSetValue(m_tls_index, data_address);
            #endif

        }

        // DOES NOT INVOKE SYSCALLS , JUST ACCESSES SEGMENT REGISTERS
        // SO IDEAL FOR IDENTIFIYING THREADS THAT USE TLS
        static inline uint64_t get_thread_local_storage_id(void)
        {
            uint64_t result;
            #ifdef _WIN32
            // GS
            result = reinterpret_cast<uint64_t>(NtCurrentTeb());
            #elif __linux__
            // FS
            asm volatile("mov %%fs:0, %0" : "=r"(result));
            #endif

            return result;
        }

    private:
            #if __linux__
            pthread_key_t m_tls_index = 0;
            #elif _WIN32
            unsigned long m_tls_index = 0;
            #endif

            ThreadLocalStorage() = default;
            ~ThreadLocalStorage() = default;

            ThreadLocalStorage(const ThreadLocalStorage& other) = delete;
            ThreadLocalStorage& operator= (const ThreadLocalStorage& other) = delete;
            ThreadLocalStorage(ThreadLocalStorage&& other) = delete;
            ThreadLocalStorage& operator=(ThreadLocalStorage&& other) = delete;
};

#endif
/*
    Provides :

                static unsigned int get_number_of_logical_cores()
                static unsigned int get_number_of_physical_cores()
                static bool is_hyper_threading()

                static inline void yield()
                static inline void sleep(unsigned long microseconds)

                static int get_current_core_id()
                static unsigned long get_current_thread_id()

                static int pin_calling_thread_to_cpu_core(int core_id)
                static void set_thread_name(unsigned long thread_id, const std::string_view name)
                static bool set_priority(ThreadPriority priority)

*/
#ifndef _THREAD_UTILITIES_
#define _THREAD_UTILITIES_

enum class ThreadPriority
{
    IDLE,
    BELOW_NORMAL,
    NORMAL,
    ABOVE_NORMAL,
    CRITICAL
};

struct ThreadPriorityNode
{
    ThreadPriority priority;
    int value;
};

const static std::array<ThreadPriorityNode, 5> NATIVE_THREAD_PRIORITIES =
{
    //DO POD INITIALISATION
    {
        #ifdef __linux__
        ThreadPriority::IDLE, 19,
        ThreadPriority::BELOW_NORMAL, 1,
        ThreadPriority::NORMAL, 0,
        ThreadPriority::ABOVE_NORMAL, -1,
        ThreadPriority::CRITICAL, -20
        #elif _WIN32
        ThreadPriority::IDLE, THREAD_PRIORITY_IDLE,    //-15
        ThreadPriority::BELOW_NORMAL, THREAD_PRIORITY_BELOW_NORMAL, // -1
        ThreadPriority::NORMAL, THREAD_PRIORITY_NORMAL, // 0
        ThreadPriority::ABOVE_NORMAL, THREAD_PRIORITY_ABOVE_NORMAL, // 1
        ThreadPriority::CRITICAL, THREAD_PRIORITY_TIME_CRITICAL // 15
        #endif

    }
};

/*
    Currently this module is not hybrid-architecture-aware
    Ex: P-cores and E-cores starting from Alder Lake
    That means all methods assume that all CPU cores are identical
*/

class ThreadUtilities
{
    public:

        static constexpr inline int MAX_THREAD_NAME_LENGTH = 16; // Limitation comes from Linux

        static unsigned int get_current_pid()
        {
            unsigned int ret{ 0 };
            #ifdef __linux__
            ret = static_cast<unsigned int>(getpid());
            #elif _WIN32
            ret = static_cast<unsigned int>(GetCurrentProcessId());
            #endif

            return ret;
        }

        static unsigned int get_number_of_logical_cores()
        {
            unsigned int num_cores{0};
            #ifdef __linux__
            num_cores = sysconf(_SC_NPROCESSORS_ONLN);
            #elif _WIN32
            SYSTEM_INFO sysinfo;
            GetSystemInfo(&sysinfo);
            num_cores = sysinfo.dwNumberOfProcessors;
            #endif

            return num_cores;
        }

        static unsigned int get_number_of_physical_cores()
        {
            auto num_logical_cores = get_number_of_logical_cores();
            bool cpu_hyperthreading = is_hyper_threading();
            return cpu_hyperthreading ? num_logical_cores / 2 : num_logical_cores;
        }

        static bool is_hyper_threading()
        {
            bool ret = false;

            #ifdef __linux__
            // Using syscalls to avoid dynamic memory allocation
            int file_descriptor = open("/sys/devices/system/cpu/smt/active", O_RDONLY);

            if (file_descriptor != -1)
            {
                char value;
                if (read(file_descriptor, &value, sizeof(value)) > 0)
                {
                    int smt_active = value - '0';
                    ret = (smt_active > 0);
                }

                close(file_descriptor);
            }
            #elif _WIN32
            SYSTEM_INFO sys_info;
            GetSystemInfo(&sys_info);
            char buffer[2048]; // It may be insufficient however even if one logical processor has SMT flag , it means we are hyperthreading
            DWORD buffer_size = sizeof(buffer);

            GetLogicalProcessorInformation(reinterpret_cast<SYSTEM_LOGICAL_PROCESSOR_INFORMATION*>(&buffer), &buffer_size);

            DWORD num_system_logical_processors = buffer_size / sizeof(SYSTEM_LOGICAL_PROCESSOR_INFORMATION);
            for (DWORD i = 0; i < num_system_logical_processors; ++i)
            {
                if (reinterpret_cast<SYSTEM_LOGICAL_PROCESSOR_INFORMATION*>(&buffer[i])->Relationship == RelationProcessorCore)
                {
                    if (reinterpret_cast<SYSTEM_LOGICAL_PROCESSOR_INFORMATION*>(&buffer[i])->ProcessorCore.Flags == LTP_PC_SMT)
                    {
                        ret = true;
                        break;
                    }
                }
            }
            #endif

            return ret;
        }

        static inline void yield()
        {
            #ifdef __linux__
            sched_yield();
            #elif _WIN32
            SwitchToThread();
            #endif

        }

        static inline void sleep(unsigned long microseconds)
        {
            #ifdef __linux__
            usleep(microseconds);
            #elif _WIN32
            // In Windows , you can sleep in terms of milliseconds...
            auto iterations = microseconds / 1000;
            for (unsigned long i{ 0 }; i < iterations; i++)
            {
                Sleep(1);
            }
            #endif

        }

        static int pin_calling_thread_to_cpu_core(int core_id)
        {
            int ret{ -1 };
            #ifdef __linux__
            cpu_set_t cpuset;
            CPU_ZERO(&cpuset);
            CPU_SET(core_id, &cpuset);
            ret = pthread_setaffinity_np(pthread_self(), sizeof(cpu_set_t), &cpuset);
            #elif _WIN32
            unsigned long mask = 1 << (core_id);

            if (SetThreadAffinityMask(GetCurrentThread(), mask))
            {
                ret = 0;
            }
            #endif

            return ret;
        }

        static int get_current_core_id()
        {
            int current_core_id{ -1 };
            #ifdef __linux__
            current_core_id = ::sched_getcpu();
            #elif _WIN32
            current_core_id = ::GetCurrentProcessorNumber();
            #endif

            return current_core_id;
        }

        static unsigned long get_current_thread_id()
        {
            unsigned long thread_id{ 0 };
            #ifdef __linux__
            thread_id = pthread_self();
            #elif _WIN32
            thread_id = ::GetCurrentThreadId();
            #endif

            return thread_id;
        }

        static void set_thread_name(unsigned long thread_id, const std::string_view name)
        {
            auto name_length = name.length();

            if (name_length > 0  && name_length <= MAX_THREAD_NAME_LENGTH )
            {
                #ifdef __linux__
                pthread_setname_np(thread_id, name.data());
                #elif _WIN32
                // As documented on MSDN
                // https://msdn.microsoft.com/en-us/library/xcb2z8hs(v=vs.120).aspx
                const DWORD MS_VC_EXCEPTION = 0x406D1388;

                #pragma pack(push,8)
                typedef struct tagTHREADNAME_INFO
                {
                    DWORD dwType; // Must be 0x1000.
                    LPCSTR szName; // Pointer to name (in user addr space).
                    DWORD dwThreadID; // Thread ID (-1=caller thread).
                    DWORD dwFlags; // Reserved for future use, must be zero.
                } THREADNAME_INFO;
                #pragma pack(pop)

                THREADNAME_INFO info;
                info.dwType = 0x1000;
                info.szName = name.data();
                info.dwThreadID = thread_id;
                info.dwFlags = 0;

                __try
                {
                    RaiseException(MS_VC_EXCEPTION, 0, sizeof(info) / sizeof(ULONG_PTR), (ULONG_PTR*)&info);
                }
                __except (EXCEPTION_EXECUTE_HANDLER)
                {
                }
                #endif

            }
        }

        static bool set_priority(ThreadPriority priority)
        {
            bool success{ false };
            int index = static_cast<std::underlying_type<ThreadPriority>::type>(priority);
            auto native_priority_value = NATIVE_THREAD_PRIORITIES[index].value;
            #ifdef __linux__
            struct sched_param param;
            param.__sched_priority = native_priority_value;
            int policy = sched_getscheduler(getpid());
            pthread_setschedparam(pthread_self(), policy, &param);
            #elif _WIN32
            if (SetThreadPriority(GetCurrentThread(), native_priority_value) != 0)
            {
                success = true;
            }
            #endif

            return success;
        }

    private:
};

#endif

/*
    The advantage of having this compared to just using std::mutex is that you can place breakpoints inside lock and unlock
    additionally you can further fine-tune Windows lock if necessary

    initialise & uninitialise provided to have the same interface with POD locks which can't have ctors & dtors
*/
#ifndef _LOCK_
#define _LOCK_

class Lock
{
    public:

        Lock()
        {
            initialise();
        }

        ~Lock()
        {
            uninitialise();
        }

        void initialise()
        {
            #if __linux__
            pthread_mutex_init(&m_mutex, nullptr);
            #elif _WIN32
            InitializeCriticalSection(&m_critical_section);
            #endif

        }

        void uninitialise()
        {
            #if __linux__
            pthread_mutex_destroy(&m_mutex);
            #elif _WIN32
            DeleteCriticalSection(&m_critical_section);
            #endif

        }

        void lock()
        {
            #if __linux__
            pthread_mutex_lock(&m_mutex);
            #elif _WIN32
            EnterCriticalSection(&m_critical_section);
            #endif

        }

        void unlock()
        {
            #if __linux__
            pthread_mutex_unlock(&m_mutex);
            #elif _WIN32
            LeaveCriticalSection(&m_critical_section);
            #endif

        }

    private:

        #if __linux__
        pthread_mutex_t m_mutex;
        #elif _WIN32
        CRITICAL_SECTION m_critical_section;
        #endif

};
#endif
#ifndef __ENVIRONMENT_VARIABLE__
#define __ENVIRONMENT_VARIABLE__

class EnvironmentVariable
{
    public:

        // Does not allocate memory
        template <typename T>
        static T get_variable(const char* environment_variable_name, T default_value)
        {
            T value = default_value;
            char* str_value = nullptr;

            #ifdef _WIN32
            // MSVC does not allow std::getenv due to safety
            std::size_t str_value_len;
            errno_t err = _dupenv_s(&str_value, &str_value_len, environment_variable_name);
            if (err)
                return value;
            #elif __linux__
            str_value = std::getenv(environment_variable_name);
            #endif

            if (str_value)
            {
                if constexpr (std::is_arithmetic<T>::value)
                {
                    char* end_ptr = nullptr;
                    auto current_val = std::strtold(str_value, &end_ptr);

                    if (*end_ptr == '\0')
                    {
                        value = static_cast<T>(current_val);
                    }
                }
                else if constexpr (std::is_same<T, char*>::value || std::is_same<T, const char*>::value)
                {
                    value = str_value;
                }
            }

            return value;
        }

        // Utility function when handling csv numeric parameters from environment variables, does not allocate memory
        static void set_numeric_array_from_comma_separated_value_string(std::size_t* target_array, const char* str)
        {
            auto len = strlen(str);

            constexpr std::size_t MAX_STRING_LEN = 64;
            constexpr std::size_t MAX_TOKEN_LEN = 8;
            std::size_t start = 0;
            std::size_t end = 0;
            std::size_t counter = 0;

            while (end <= len && end < MAX_STRING_LEN - 1)
            {
                if (str[end] == ',' || (end > start && end == len))
                {
                    char token[MAX_TOKEN_LEN];
                    std::size_t token_len = end - start;

                    #ifdef __linux__
                    strncpy(token, str + start, token_len);
                    #elif _WIN32
                    strncpy_s(token, str + start, token_len);
                    #endif

                    token[token_len] = '\0';

                    target_array[counter] = atoi(token);

                    start = end + 1;
                    counter++;
                }

                ++end;
            }
        }
};

#endif
#ifndef __TRACE_H__
#define __TRACE_H__

#ifdef ENABLE_TRACER

// DOES NOT ALLOCATE MEMORY
void trace(const char* format, ...)
{
    const std::size_t max_length = 1024;
    char buffer[max_length];

    va_list args;
    va_start(args, format);
    std::size_t length = vsnprintf(buffer, max_length, format, args);
    va_end(args);

    if (length < 0)
    {
        return;
    }
    #ifdef __linux__
    auto ret = write(STDOUT_FILENO, buffer, length);
    #elif _WIN32
    OutputDebugStringA(buffer);
    #endif

    UNUSED(ret);
}

#define trace_message(MESSAGE) (trace(("%s\n"), (MESSAGE)));
#define trace_integer_value(VAR_NAME,VALUE); (trace(("variable name = %s , value=%zu\n"), (VAR_NAME),(VALUE)));
#define trace_double_value(VAR_NAME,VALUE); (trace(("variable name = %s , value=%f\n"), (VAR_NAME),(VALUE)));
#define trace_string_value(VAR_NAME,VALUE); (trace(("variable name = %s , value=%s\n"), (VAR_NAME),(VALUE)));

#else
// COMPILER WON'T GENERATE CODE FOR EXISTING CALLS
#define trace_message(MESSAGE);
#define trace_integer_value(VAR_NAME,VALUE);
#define trace_double_value(VAR_NAME,VALUE);
#define trace_string_value(VAR_NAME,VALUE);
#endif

#endif
#ifndef __ALIGNMENT_CHECKS_H__
#define __ALIGNMENT_CHECKS_H__

class AlignmentChecks
{
    public:

        static bool is_address_aligned(void* address, std::size_t alignment)
        {
            std::size_t alignment_mask = alignment - 1;
            std::size_t address_value = reinterpret_cast<std::size_t>(address);
            return (address_value & alignment_mask) == 0;
        }

        static bool is_address_page_allocation_granularity_aligned(void* address)
        {
            return is_address_aligned(address, VirtualMemory::PAGE_ALLOCATION_GRANULARITY);
        }
};

#endif
#ifndef __LOG2_UTILITIES_H__
#define __LOG2_UTILITIES_H__

class Log2Utilities
{
    public:

        static constexpr unsigned int compile_time_log2(unsigned int n)
        {
            return (n <= 1) ? 0 : 1 + compile_time_log2(n / 2);
        }

        static std::size_t log2_power_of_two(std::size_t input)
        {
            // IMPLEMENTATION IS FOR 64 BIT ONLY
            return 63 - builtin_clzl(static_cast<unsigned long>(input));
        }

        static std::size_t log2(std::size_t n)
        {
            std::size_t result = 0;
            while (n >>= 1)
            {
                ++result;
            }
            return result;
        }
};

#endif
#ifndef __POW2_UTILITIES_H__
#define __POW2_UTILITIES_H__

class Pow2Utilities
{
    public:

        template <std::size_t N>
        static constexpr std::size_t compile_time_pow2()
        {
            return 1 << N;
        }

        // Reference : https://graphics.stanford.edu/~seander/bithacks.html#RoundUpPowerOf2
        static std::size_t get_first_pow2_of(std::size_t input)
        {
            if (input <= 1)
            {
                return 1;
            }

            input--;
            input |= input >> 1;
            input |= input >> 2;
            input |= input >> 4;
            input |= input >> 8;
            input |= input >> 16;

            return input + 1;
        }

        static bool is_power_of_two(std::size_t input)
        {
            if (input == 0)
            {
                return false;
            }

            return (input & (input - 1)) == 0;
        }

        template <std::size_t N>
        static constexpr bool compile_time_is_power_of_two()
        {
            return N && ((N & (N - 1)) == 0);
        }
};

#endif
#ifndef __MODULO_UTILITIES_H__
#define __MODULO_UTILITIES_H__

class ModuloUtilities
{
    public:
        // "second" should be power of 2
        static std::size_t modulo_pow2(std::size_t first, std::size_t second)
        {
            assert(Pow2Utilities::is_power_of_two(second));
            return first & (second - 1);
        }

        static std::size_t modulo(std::size_t first, std::size_t second)
        {
            std::size_t shift = builtin_ctzl(static_cast<unsigned long>(second));
            std::size_t remainder = first - ((first >> shift) << shift);
            return remainder;
        }
};

#endif
#ifndef __MULTIPLE_UTILITIES_H__
#define __MULTIPLE_UTILITIES_H__

class MultipleUtilities
{
    public:

        // "multiple" should be power of 2
        static std::size_t get_next_pow2_multiple_of(std::size_t input, std::size_t multiple)
        {
            // Not checking if the given input is already a multiple , as "next" means that
            // we are called from a place that will add 'further bytes to that will 'grow' therefore no need to check
            assert(Pow2Utilities::is_power_of_two(multiple));
            return ((input + multiple - 1) & ~(multiple - 1));
        }

        static bool is_size_a_multiple_of_page_allocation_granularity(std::size_t input)
        {
            return ModuloUtilities::modulo_pow2(input, VirtualMemory::PAGE_ALLOCATION_GRANULARITY) == 0;
        }
};

#endif
#ifndef __SIZE_UTILITIES_H__
#define __SIZE_UTILITIES_H__

class SizeUtilities
{
    public:

        // When your heap is size segregated , where each size is a pow2,
        // you need to calculate zero based bin index from a given size
        //
        // Ex: if size is 96 & min size class is 16, then we shall return 3 ( 4th bin , bin1:16s bin2:32s, bin3:64s, bin4:128s ...)
        //
        template <std::size_t MINIMUM_SIZE_CLASS, std::size_t MAX_BIN_INDEX>
        static std::size_t get_pow2_bin_index_from_size(std::size_t size)
        {
            std::size_t index = Log2Utilities::log2_power_of_two(size) - Log2Utilities::compile_time_log2(MINIMUM_SIZE_CLASS) ;
            index = index > MAX_BIN_INDEX ? MAX_BIN_INDEX : index;
            return index;
        }

        static std::size_t get_required_page_count_for_allocation(std::size_t page_size, std::size_t page_header_size, std::size_t object_size, std::size_t object_count)
        {
            std::size_t object_count_per_page = static_cast<std::size_t>(std::ceil( (page_size - page_header_size) / object_size));
            std::size_t needed_page_count = static_cast<std::size_t>(std::ceil(static_cast<double>(object_count) / static_cast<double>(object_count_per_page)));

            if (needed_page_count == 0)
            {
                needed_page_count = 1;
            }

            return needed_page_count;
        }

        #ifdef ENABLE_STATS
        static const std::string get_human_readible_size(std::size_t size_in_bytes)
        {
            const char* suffixes[] = { "B", "KB", "MB", "GB", "TB", "PB", "EB", "ZB", "YB" };
            constexpr int suffix_count = sizeof(suffixes) / sizeof(suffixes[0]);

            std::ostringstream oss;

            double size = static_cast<double>(size_in_bytes);
            constexpr int multiplier = 1024;
            int suffix_index = 0;

            while (size >= multiplier && suffix_index < suffix_count - 1)
            {
                size /= multiplier;
                suffix_index++;
            }

            oss << std::fixed << std::setprecision(2) << size << ' ' << suffixes[suffix_index];
            return oss.str();
        }
        #endif

};

#endif
/*
    A CAS ( compare-and-swap ) based POD ( https://en.cppreference.com/w/cpp/language/classes#POD_class ) spinlock
    As it is POD , it can be used inside packed declarations.

    To keep it as POD :
                1. Not using standard C++ std::atomic
                2. Member variables should be public

    Otherwise GCC will generate : warning: ignoring packed attribute because of unpacked non-POD field

    PAHOLE OUTPUT :

                size: 4, cachelines: 1, members: 1
                last cacheline: 4 bytes

    Can be faster than os/lock or std::mutex
    However should be picked carefully as it will affect all processes on a CPU core
    even though they are not doing the same computation so misuse may lead to starvation for others

    Doesn`t check against uniprocessors. Prefer "Lock" in os/lock.h for old systems
*/
#ifndef _USERSPACE_SPINLOCK_
#define _USERSPACE_SPINLOCK_

// Pass alignment = AlignmentConstants::CACHE_LINE_SIZE to make the lock cacheline aligned

template<std::size_t alignment=sizeof(uint32_t), std::size_t spin_count = 1024, std::size_t pause_count = 64, bool extra_system_friendly = false>
struct UserspaceSpinlock
{
    // No privates, ctors or dtors to stay as PACKED+POD
    ALIGN_DATA(alignment) uint32_t m_flag=0;

    void initialise()
    {
        m_flag = 0;
    }

    void lock()
    {
        while (true)
        {
            for (std::size_t i(0); i < spin_count; i++)
            {
                if (try_lock() == true)
                {
                    return;
                }

                pause(pause_count);
            }

            if constexpr (extra_system_friendly)
            {
                ThreadUtilities::yield();
            }
        }
    }

    FORCE_INLINE bool try_lock()
    {
        if (builtin_cas(&m_flag, 0, 1) == 1)
        {
            return false;
        }

        return true;
    }

    FORCE_INLINE void unlock()
    {
        m_flag = 0;
    }
};

#endif
#ifndef _LOCKABLE_H_
#define _LOCKABLE_H_

enum class LockPolicy
{
    NO_LOCK,
    OS_LOCK,
    USERSPACE_LOCK,
    USERSPACE_LOCK_CACHELINE_ALIGNED
};

// Since it is a template base class, deriving classes need "this" or full-qualification in order to call its methods
template <LockPolicy lock_policy>
class Lockable
{
public:

    using LockType = std::conditional_t<
        lock_policy == LockPolicy::OS_LOCK,
        Lock, std::conditional_t<
        lock_policy == LockPolicy::USERSPACE_LOCK_CACHELINE_ALIGNED,
        UserspaceSpinlock<AlignmentConstants::CACHE_LINE_SIZE>, UserspaceSpinlock<>>>;

    Lockable()
    {
        m_lock.initialise();
    }

    void enter_concurrent_context()
    {
        if constexpr (lock_policy != LockPolicy::NO_LOCK)
        {
            m_lock.lock();
        }
    }

    void leave_concurrent_context()
    {
        if constexpr (lock_policy != LockPolicy::NO_LOCK)
        {
            m_lock.unlock();
        }
    }
private:
    LockType m_lock;
};

#endif
/*
    THE MAIN FUNCTIONALITY HERE IS "allocate_aligned" IMPLEMENTATION : DURING DEALLOCATIONS, WE AIM FIND OUT LOGICAL PAGES OF THE ADDRESSES WHICH ARE BEING FREED BY APPLYING MODULO ON THE ADDRESS,
    SINCE HEADERS WILL IDEALLY BE PLACED ON THE VERY START OF LOGICAL PAGES. SO WE NEED LOGICAL PAGES TO BE ALIGNED TO CHOSEN LOGICAL_PAGE_SIZES.

    LINUX MMAP RETURNS ONLY 4KB-ALIGNED ADDRESSES AND WINDOWS VIRTUALALLOC ONLY 64KB-ALIGNED ADDRESSES. ( AS THAT IS PAGE ALLOC GRANULARITY ON WINDOWS )
    THEY DON'T GUARANTEE ALIGNMENTS APART FROM 4KB & 64KB. THEREFORE ARENA HANDLES THE ALIGNMENT FOR ARBITRARY ALIGNMENTS BEYOND 4KB & 64 KB WITH OVER-SIZED VM ALLOCATIONS
*/

#ifndef __ARENA_BASE_H__
#define __ARENA_BASE_H__

template <typename ArenaImplementation>
class ArenaBase
{
    public:
        ArenaBase() = default;
        ~ArenaBase() = default;
        ArenaBase(const ArenaBase& other) = delete;
        ArenaBase& operator= (const ArenaBase& other) = delete;
        ArenaBase(ArenaBase&& other) = delete;
        ArenaBase& operator=(ArenaBase&& other) = delete;

        [[nodiscard]] void* allocate_from_system(std::size_t size) { return static_cast<ArenaImplementation*>(this)->allocate_from_system(size); }
        void release_to_system(void* address, std::size_t size) { static_cast<ArenaImplementation*>(this)->release_to_system(address, size); }

    protected:

        char* allocate_aligned(std::size_t size, std::size_t alignment)
        {
            std::size_t actual_size = size + alignment;
            char* buffer{ nullptr };

            buffer = static_cast<char*>(allocate_from_system(actual_size));

            if (buffer == nullptr)
            {
                return nullptr;
            }

            std::size_t remainder = ModuloUtilities::modulo( reinterpret_cast<std::size_t>(buffer), alignment);
            std::size_t delta = 0;

            if (remainder > 0)
            {
                // WE NEED PADDING FOR SPECIFIED PAGE ALIGNMENT
                delta = alignment - remainder;
                // RELEASING PADDING PAGE
                release_to_system(buffer, delta);
            }
            else
            {
                // PADDING IS NOT NEEDED, HENCE THE EXTRA ALLOCATED PAGE IS EXCESS
                release_to_system(buffer + actual_size - alignment, alignment);
            }
            return buffer + delta;
        }
};

#endif
/*
    - ARENA ABSTRACTION REDUCES SYSCALLS BY CACHING AND SERVING VIRTUAL MEMORY PAGES.
      ( SOME ALLOCATORS LIKE JEMALLOC USES THE ARENA TERM DIFFERENTLY : AS THE HIGHEST LAYER ALLOCATOR WHICH OWNS MULTIPLE HEAPS.
        SHOULDN'T BE CONFUSED WITH THAT )

    - ARENA CLASS IS PAGE BASED ALLOCATOR AND USED TO ALLOCATE LOGICAL_PAGE BUFFERS WITHIN SEGMENTS.

    - IT RELEASES ONLY UNUSED PAGES. RELEASING USED PAGES IS UP TO THE CALLERS. ( THIS HELPS TO REDUCE METADATA USAGE BY AVOIDING A BITMAP )

    - IF HUGE PAGE IS SPECIFIED AND A HUGE PAGE ALLOCATION FAILS, FAILOVERS TO REGULAR PAGE ALLOCATION

    - SUPPORTS NO LOCKING AND LOCKING (EITHER OS_LOCK OR USERSPACE_SPINLOCK) SO THAT IT CAN BE USED BY A SINGLE HEAP OR SHARED BY MULTIPLE HEAPS

    - lock_pages & unlock_pages METHODS CAN BE USED SO THAT THE SYSTEM WILL NOT SWAP PAGES TO THE PAGING FILE

    - LINUX ALLOCATION GRANULARITY IS 4KB (4096) , OTH IT IS 64KB ( 16 * 4096 ) ON WINDOWS .
      REGARDING WINDOWS PAGE ALLOCATION GRANULARITY : https://devblogs.microsoft.com/oldnewthing/20031008-00/?p=42223
      FOR THEIR ALIGNMENTS SEE COMMENTS IN ARENA_BASE.H.
*/
#ifndef _ARENA_H_
#define _ARENA_H_

enum class VirtualMemoryPolicy
{
    DEFAULT,
    HUGE_PAGE,                // MAY BE MORE LOAD TO THE SYSTEM BUT REDUCES TLB MISSES AND MAKE VIRTUAL MEM PAGE MANAGEMENT LESS EXPENSIVE
    #ifdef UNIT_TEST
    OUT_OF_MEMORY            // FOR UNIT TESTS : TESTS THE CASE WHEN THERE IS NO AVAILABLE SYSTEM MEMORY
    #endif

};

#ifdef ENABLE_STATS
constexpr static inline std::size_t MAX_ALLOC_STAT_COUNT = 32;
struct ArenaStats
{
    std::size_t m_vm_allocation_count = 0;
    std::array<std::size_t, MAX_ALLOC_STAT_COUNT> m_vm_allocation_sizes = { 0 };
    std::size_t m_latest_used_size = 0;
};
#endif

// MAINTAINS A SHARED CACHE THEREFORE LOCKED BY DEFAULT
template <LockPolicy lock_policy = LockPolicy::USERSPACE_LOCK, VirtualMemoryPolicy virtual_memory_policy = VirtualMemoryPolicy::DEFAULT, std::size_t numa_node = VirtualMemory::NO_NUMA, bool zero_memory = false>
class Arena : public Lockable<lock_policy>, public ArenaBase<Arena<lock_policy, virtual_memory_policy, numa_node, zero_memory>>
{
    public:

        Arena()
        {
            m_vm_page_size = VirtualMemory::get_page_size(); // DEFAULT VALUE
            m_page_alignment = VirtualMemory::PAGE_ALLOCATION_GRANULARITY;
        }

        ~Arena()
        {
            destroy();
        }

        Arena(const Arena& other) = delete;
        Arena& operator= (const Arena& other) = delete;
        Arena(Arena&& other) = delete;
        Arena& operator=(Arena&& other) = delete;

        [[nodiscard]] bool create(std::size_t cache_capacity, std::size_t page_alignment)
        {
            if (MultipleUtilities::is_size_a_multiple_of_page_allocation_granularity(m_page_alignment) == false)
            {
                return false;
            }

            this->enter_concurrent_context();
            //////////////////////////////////////////////////
            m_page_alignment = page_alignment;
            auto ret =  build_cache(cache_capacity);
            //////////////////////////////////////////////////
            this->leave_concurrent_context();

            return ret;
        }

        void destroy()
        {
            if (m_cache_size > m_cache_used_size)
            {
                // ARENA IS RESPONSIBLE OF CLEARING ONLY NEVER-REQUESTED PAGES.
                std::size_t release_start_address = reinterpret_cast<std::size_t>(m_cache_buffer + m_cache_used_size);
                std::size_t release_end_address = reinterpret_cast<std::size_t>(m_cache_buffer + m_cache_size);

                for (; release_start_address < release_end_address; release_start_address += m_vm_page_size)
                {
                    release_to_system(reinterpret_cast<void *>(release_start_address), m_vm_page_size);
                }

            }
            m_cache_size = 0;
            m_cache_used_size = 0;
            m_cache_buffer = nullptr;
        }

        [[nodiscard]] char* allocate(std::size_t size)
        {
            this->enter_concurrent_context();
            //////////////////////////////////////////////////
            if (size + m_page_alignment > (m_cache_size - m_cache_used_size))
            {
                destroy();

                if (!build_cache(size))
                {
                    return nullptr;
                }
            }

            auto ret = m_cache_buffer + m_cache_used_size;
            m_cache_used_size += size;
            //////////////////////////////////////////////////
            this->leave_concurrent_context();

            return ret;
        }

        std::size_t page_size()const { return m_vm_page_size; }
        std::size_t page_alignment() const { return m_page_alignment; }

        void lock_pages()
        {
            uint64_t address = reinterpret_cast<uint64_t>(m_cache_buffer);
            uint64_t end_address = reinterpret_cast<uint64_t>(m_cache_buffer + m_cache_size);

            for (; address < end_address; address += m_vm_page_size)
            {
                VirtualMemory::lock(reinterpret_cast<void*>(address), m_vm_page_size);
            }
        }

        void unlock_pages()
        {
            uint64_t address = reinterpret_cast<uint64_t>(m_cache_buffer);
            uint64_t end_address = reinterpret_cast<uint64_t>(m_cache_buffer + m_cache_size);

            for (; address < end_address; address += m_vm_page_size)
            {
                VirtualMemory::unlock(reinterpret_cast<void*>(address), m_vm_page_size);
            }
        }

        void* allocate_from_system(std::size_t size)
        {
            void* ret = nullptr;

            if constexpr (virtual_memory_policy == VirtualMemoryPolicy::DEFAULT)
            {
                ret = static_cast<char*>(VirtualMemory::allocate<false, numa_node, zero_memory>(size, nullptr));
            }
            else if  constexpr (virtual_memory_policy == VirtualMemoryPolicy::HUGE_PAGE)
            {
                ret = static_cast<char*>(VirtualMemory::allocate<true, numa_node, zero_memory>(size, nullptr));

                // If huge page fails, try regular ones
                if (ret == nullptr)
                {
                    ret = static_cast<char*>(VirtualMemory::allocate<false, numa_node, zero_memory>(size, nullptr));
                }
            }

            return ret;
        }

        void release_to_system(void* address, std::size_t size)
        {
            VirtualMemory::deallocate(address, size);
        }

        class MetadataAllocator
        {
            public:
                static void* allocate(std::size_t size, void* hint_address = nullptr)
                {
                    return VirtualMemory::allocate<false>(size, hint_address); // No hugepage, no NUMA and no zeroing
                }

                static void deallocate(void* address, std::size_t size)
                {
                    VirtualMemory::deallocate(address, size);
                }
        };

        #ifdef ENABLE_STATS
        ArenaStats get_stats() { m_stats.m_latest_used_size = m_cache_used_size;  return m_stats; }
        #endif

    private:
        std::size_t m_vm_page_size = 0;
        std::size_t m_page_alignment = 0;
        char* m_cache_buffer = nullptr;
        std::size_t m_cache_size = 0;
        std::size_t m_cache_used_size = 0;

        #ifdef ENABLE_STATS
        ArenaStats m_stats;
        #endif

        [[nodiscard]] bool build_cache(std::size_t size)
        {
            char* buffer = this->allocate_aligned(size, m_page_alignment);

            if (buffer == nullptr)
            {
                return false;
            }

            #ifdef ENABLE_PERF_TRACES // INSIDE ALLOCATION CALLSTACK SO CAN'T ALLOCATE MEMORY HENCE OUTPUT TO stderr
            fprintf(stderr, "arena build cache virtual memory allocation , size=%zu\n", size);
            #endif

            #ifdef ENABLE_STATS
            if(m_stats.m_vm_allocation_count<MAX_ALLOC_STAT_COUNT)
            {
                m_stats.m_vm_allocation_sizes[m_stats.m_vm_allocation_count] = size;
                m_stats.m_vm_allocation_count++;
            }
            #endif

            m_cache_buffer = buffer;
            m_cache_used_size = 0;
            m_cache_size = size;

            return true;
        }
};

#endif
/*
    POD LOGICAL PAGE HEADER
    LOGICAL PAGE HEADERS WILL BE PLACED TO THE FIRST 64 BYTES OF EVERY LOGICAL PAGE.

    WE USE 2 BYTES FOR PADDING AS THIS IS TO ENSURE THAT ALL ALLOCATIONS WILL BE AT LEAST 16-BIT ALIGNED

    PAHOLE OUTPUT :
                            size: 64, cachelines: 1, members: 10
                            last cacheline: 64 bytes
*/
#ifndef __LOGICAL_PAGE_HEADER_H__
#define __LOGICAL_PAGE_HEADER_H__

enum class LogicalPageHeaderFlags : uint16_t
{
    IS_USED = 0x0001,
    IS_DIRTY = 0x0002,
    IS_LOCKED = 0x0004,
    IS_HUGE_PAGE = 0x0008
};

PACKED
(
    struct LogicalPageHeader // No privates , member initialisers, ctors or dtors to stay as PACKED+POD
    {
            // 8 BYTES
            uint64_t m_head;                    // Freelist to track memory
            // 8 BYTES
            uint64_t m_next_logical_page_ptr;  // To be used by an upper layer abstraction (ex: segment span etc ) to navigate between logical pages
            // 8 BYTES
            uint64_t m_prev_logical_page_ptr;  // Same as above
            // 2 BYTES
            uint16_t m_page_flags;               // See enum class LogicalPageHeaderFlags
            // 4 BYTES
            uint32_t m_size_class;               // Used to distinguish non-big size class pages    , since logical pages won't be holding objects > page size, 2 bytes will be sufficient
            // 8 BYTES
            uint64_t m_used_size;
            // 8 BYTES
            uint64_t m_logical_page_start_address;
            // 8 BYTES
            uint64_t m_logical_page_size;
            // 8 BYTES
            uint64_t m_last_used_node;
            // 2 BYTES
            char m_padding_bytes[2];

            // Total = 64 bytes

            void initialise()
            {
                m_head = 0;
                m_next_logical_page_ptr = 0;
                m_prev_logical_page_ptr = 0;
                m_page_flags = 0;
                m_size_class = 0;
                m_used_size = 0;
                m_logical_page_start_address = 0;
                m_logical_page_size = 0;
                m_last_used_node = 0;
            }

            template<LogicalPageHeaderFlags flag>
            void set_flag()
            {
                m_page_flags |= static_cast<uint16_t>(flag);
            }

            template<LogicalPageHeaderFlags flag>
            void clear_flag()
            {
                m_page_flags &= ~static_cast<uint16_t>(flag);
            }

            template<LogicalPageHeaderFlags flag>
            bool get_flag() const
            {
                return (m_page_flags & static_cast<uint16_t>(flag)) != 0;
            }
    }
);

#endif
//BASE CLASS AND INTERFACE FOR LOGICAL PAGES
#ifndef __LOGICAL_PAGE_BASE_H__
#define __LOGICAL_PAGE_BASE_H__

template <typename LogicalPageImplementation, typename NodeType>
class LogicalPageBase
{
    public:
        LogicalPageBase()
        {
            m_page_header.initialise();
        }

        ~LogicalPageBase() {}

        LogicalPageBase(const LogicalPageBase& other) = delete;
        LogicalPageBase& operator= (const LogicalPageBase& other) = delete;
        LogicalPageBase(LogicalPageBase&& other) = delete;
        LogicalPageBase& operator=(LogicalPageBase&& other) = delete;

        bool owns_pointer(void* ptr)
        {
            uint64_t address_in_question = reinterpret_cast<uint64_t>(ptr);

            if ( address_in_question >= m_page_header.m_logical_page_start_address && address_in_question < m_page_header.m_logical_page_start_address + m_page_header.m_logical_page_size )
            {
                return true;
            }

            return false;
        }

        bool can_be_recycled() { return m_page_header.get_flag<LogicalPageHeaderFlags::IS_USED>() == false; }

        void mark_as_used() { m_page_header.set_flag<LogicalPageHeaderFlags::IS_USED>();  }
        void mark_as_non_used() { m_page_header.clear_flag<LogicalPageHeaderFlags::IS_USED>(); }

        void mark_as_locked() { m_page_header.set_flag<LogicalPageHeaderFlags::IS_LOCKED>(); }
        void mark_as_non_locked() { m_page_header.clear_flag<LogicalPageHeaderFlags::IS_LOCKED>(); }

        uint64_t get_used_size() const { return m_page_header.m_used_size; }
        uint32_t get_size_class() { return m_page_header.m_size_class; }

        uint64_t get_next_logical_page() const { return m_page_header.m_next_logical_page_ptr; }
        void set_next_logical_page(void* address) { m_page_header.m_next_logical_page_ptr = reinterpret_cast<uint64_t>(address); }

        uint64_t get_previous_logical_page() const { return m_page_header.m_prev_logical_page_ptr; }
        void set_previous_logical_page(void* address) { m_page_header.m_prev_logical_page_ptr = reinterpret_cast<uint64_t>(address); }

        #ifdef UNIT_TEST
        const std::string get_type_name() const { return static_cast<LogicalPageImplementation*>(this)->get_type_name(); }
        std::size_t capacity() const { return m_capacity; }
        NodeType* get_head_node() { return reinterpret_cast<NodeType*>(m_page_header.m_head); };
        #endif

    protected:
        LogicalPageHeader m_page_header;

        #ifdef UNIT_TEST
        std::size_t m_capacity = 0;
        #endif

};

#endif

/*
    - IT IS A FIRST-IN-LAST-OUT FREELIST IMPLEMENTATION. IT CAN HOLD ONLY ONE SIZE CLASS. ALLOCATE METHOD WILL IGNORE THE SIZE PARAMETER

    - IF THE PASSED BUFFER IS START OF A VIRTUAL PAGE AND THE PASSED SIZE IS A VM PAGE SIZE , THEN IT WILL BE CORRESPONDING TO AN ACTUAL VM PAGE
      IDEAL USE CASE IS ITS CORRESPONDING TO A VM PAGE / BEING VM PAGE ALIGNED. SO THAT A SINGLE PAYLOAD WILL NOT SPREAD TO DIFFERENT VM PAGES

    - IF THE PASSED BUFFER SIZE IS A MULTIPLE OF VM PAGE SIZE, THEN IT CAN HOLD MULTIPLE CONTIGUOUS VM PAGES, SIMILARLY TO SPANS/PAGE RUNS

    - MINIMUM ALLOCATION SIZE IS 8 BYTES. ( THAT IS BECAUSE A POINTER IN 64 BIT IS 8 BYTES )

    - DOESN'T SUPPORT ALIGNMENT. ALLOCATE METHOD WILL IGNORE THE ALIGNMENT PARAMETER

    - METADATA USAGE : 32 BYTES (PAGE HEADER) PER LOGICAL PAGE
*/
#ifndef __LOGICAL_PAGE_H__
#define __LOGICAL_PAGE_H__

PACKED
(
    struct LogicalPageNode      // No private members/method to stay as POD+PACKED
    {
        LogicalPageNode* m_next = nullptr;      // When not allocated , first 8 bytes will hold address of the next node
                                                // When allocated , 8 bytes + chunksize-8 bytes will be available to hold data
    }
);

template <typename NodeType = LogicalPageNode>
class LogicalPage : public LogicalPageBase<LogicalPage<NodeType>, LogicalPageNode>
{
    public:
        LogicalPage() {}
        ~LogicalPage() {}

        LogicalPage(const LogicalPage& other) = delete;
        LogicalPage& operator= (const LogicalPage& other) = delete;
        LogicalPage(LogicalPage&& other) = delete;
        LogicalPage& operator=(LogicalPage&& other) = delete;

        // Gets its memory from an external source such as a heap's arena
        [[nodiscard]] bool create(void* buffer, const std::size_t buffer_size, uint32_t size_class)
        {
            // Chunk size can't be smaller than a 'next' pointer-or-offset which is 64bit
            if (buffer == nullptr || buffer_size < size_class || size_class < sizeof(uint64_t))
            {
                return false;
            }

            void* buffer_start_including_header = reinterpret_cast<void*>(reinterpret_cast<std::size_t>(buffer) - sizeof(*this)); // In case a caller is placing this object instance and the memory held by this instance sequentially

            if (!AlignmentChecks::is_address_page_allocation_granularity_aligned(buffer) && !AlignmentChecks::is_address_page_allocation_granularity_aligned(buffer_start_including_header))
            {
                return false; // You have to pass start of actual virtual memory pages which will be page size aligned
            }

            this->m_page_header.initialise();
            this->m_page_header.m_size_class = size_class;
            this->m_page_header.m_logical_page_start_address = reinterpret_cast<uint64_t>(buffer);
            this->m_page_header.m_logical_page_size = buffer_size;

            #ifdef UNIT_TEST
            this->m_capacity = buffer_size;
            #endif

            grow(buffer, buffer_size);

            return true;
        }

        // We don't use size as we always allocate a fixed size chunk
        // We need the paramaters in the method to conform the common logical page interface
        ALIGN_CODE(AlignmentConstants::CACHE_LINE_SIZE) [[nodiscard]]
        void* allocate(const std::size_t size)
        {
            UNUSED(size);
            NodeType* free_node = pop();

            if (unlikely(free_node == nullptr))
            {
                return nullptr;
            }

            this->m_page_header.m_used_size += this->m_page_header.m_size_class;

            return  reinterpret_cast<void*>(free_node);
        }

        ALIGN_CODE(AlignmentConstants::CACHE_LINE_SIZE)
        void deallocate(void* ptr)
        {
            if( unlikely(this->owns_pointer(ptr) == false) )
            {
                return;
            }

            this->m_page_header.m_used_size -= this->m_page_header.m_size_class;
            push(static_cast<NodeType*>(ptr));
        }

        std::size_t get_usable_size(void* ptr) { return  static_cast<std::size_t>(this->m_page_header.m_size_class); }
        static constexpr bool supports_any_size() { return false; }

        #ifdef UNIT_TEST
        const std::string get_type_name() const { return "LogicalPage"; }
        #endif

    private:

        void grow(void* buffer, std::size_t buffer_size)
        {
            const std::size_t chunk_count = buffer_size / this->m_page_header.m_size_class;

            for (std::size_t i = 0; i < chunk_count; ++i)
            {
                std::size_t address = reinterpret_cast<std::size_t>(buffer) + i * this->m_page_header.m_size_class;
                push(reinterpret_cast<NodeType*>(address));
            }
        }

        FORCE_INLINE void push(NodeType* new_node)
        {
            new_node->m_next = reinterpret_cast<NodeType*>(this->m_page_header.m_head);
            this->m_page_header.m_head = reinterpret_cast<uint64_t>(new_node);
        }

        FORCE_INLINE NodeType* pop()
        {
            if(unlikely(this->m_page_header.m_head == 0))
            {
                return nullptr;
            }

            NodeType* top = reinterpret_cast<NodeType*>(this->m_page_header.m_head);
            this->m_page_header.m_head = reinterpret_cast<uint64_t>(top->m_next);
            return top;
        }
};

#endif
/*
- IT IS A FREELIST IMPLEMENTATION THAT CAN HOLD MULTIPLE SIZES.
  COMPARED TO LOGICAL_PAGE , IT NEEDS MORE MEMORY AND ALSO IT IS SLOWER. HOWEVER IT IS USEFUL IN 2 CASES :

                        1. AS A CENTRAL SINGLE FREELIST FOR BIG-OBJECTS THAT DON'T FIT A VM PAGE AS THEIR SIZES WILL VARY

                        2. WHENEVER HIGH-LOCALITY NEEDED BY AVOIDING SIZE-BASED SEGREGATION,  EX: LOCAL ALLOCATIONS FOR SINGLE THREADED LOW LATENCY APPS

- IF THE PASSED BUFFER IS START OF A VIRTUAL PAGE AND THE PASSED SIZE IS A VM PAGE SIZE , THEN IT WILL BE CORRESPONDING TO AN ACTUAL VM PAGE
  IDEAL USE CASE IS ITS CORRESPONDING TO A VM PAGE / BEING VM PAGE ALIGNED . SO THAT A SINGLE PAYLOAD WILL NOT SPREAD TO DIFFERENT VM PAGES

- IF THE PASSED BUFFER SIZE IS A MULTIPLE OF VM PAGE SIZE, THEN IT CAN HOLD MULTIPLE CONTIGUOUS VM PAGES, SIMILARLY TO SPANS/PAGE RUNS

- USES 16 BYTE ALLOCATION HEADER. HEDERS ARE PLACED JUST BEFORE THE PAYLOADS. DEALLOCATIONS USE THE INFO IN THOSE ALLOCATION HEADERS

- ALWAYS RETURNS 16 BYTE ALIGNED POINTERS

- IF THE PASSED BUFFER SIZE IS SAME AS TYPICAL VM PAGE SIZE -> 4KB = 4096 bytes , THEN IT CAN HOLD :

                                                                        metadata_percentage_for_node16
                                        128 * 32-BYTE   BLOCKS                50%                                (WORST CASE)
                                        64  * 64-BYTE   BLOCKS                25%
                                        32  * 128-BYTE  BLOCKS                12.5%
                                        16  * 256-BYTE  BLOCKS                6.25%
                                        8   * 512-BYTE  BLOCKS                3.12%
                                        4   * 1024-BYTE BLOCKS                1.6 %
                                        2   * 2048-BYTE BLOCKS                0.8%
                                        1   * 4096-BYTE BLOCK                 0.4%
*/
#ifndef __LOGICAL_PAGE_ANYSIZE_H__
#define __LOGICAL_PAGE_ANYSIZE_H__

// When memory is not allocated a Node will act as a freelist node
// And the same area will represent an allocation header after an allocation.
// Then if it is freed ,it will represent a freelist node again.

// No privates , member initialisers, ctors or dtors to stay as PACKED+POD
PACKED(
    struct LogicalPageAnySizeNode
    {                             // FREELIST_NODE_USAGE      ALLOCATION_HEADER_USAGE
        uint64_t m_first_part;    // next ptr/address         padding
        uint64_t m_second_part;   // block size               block size

        uint64_t get_next_pointer()
        {
            return m_first_part;
        }

        void set_next_pointer(uint64_t ptr)
        {
            m_first_part = ptr;
        }

        void clear_next_pointer()
        {
            m_first_part = 0;
        }

        std::size_t get_block_size() const
        {
            return m_second_part;
        }

        void set_block_size(std::size_t val)
        {
            m_second_part = val;
        }

        std::size_t get_padding_size() const
        {
            return m_first_part;
        }

        void set_padding_size(std::size_t val)
        {
            m_first_part = val;
        }

        // Not using std::numeric_limits as it needs extra #define or #undef on Windows
        static constexpr uint64_t MAX_BLOCK_SIZE = 0xFFFFFFFFFFFFFFFF;
        static constexpr std::size_t MIN_ALIGNMENT_SIZE = 16;
    }
);

enum class CoalescePolicy
{
    COALESCE,
    NO_COALESCING
};

template <CoalescePolicy coalesce_policy= CoalescePolicy::COALESCE>
class LogicalPageAnySize : public LogicalPageBase<LogicalPageAnySize<coalesce_policy>, LogicalPageAnySizeNode>
{
public:

    LogicalPageAnySize() {}
    ~LogicalPageAnySize() {}

    LogicalPageAnySize(const LogicalPageAnySize& other) = delete;
    LogicalPageAnySize& operator= (const LogicalPageAnySize& other) = delete;
    LogicalPageAnySize(LogicalPageAnySize&& other) = delete;
    LogicalPageAnySize& operator=(LogicalPageAnySize&& other) = delete;

    using NodeType = LogicalPageAnySizeNode;

    // Gets its memory from an external source such as a heap's arena
    [[nodiscard]] bool create(void* buffer, const std::size_t buffer_size, uint32_t size_class = 0)
    {
        if ( static_cast<uint64_t>(buffer_size) > NodeType::MAX_BLOCK_SIZE)
        {
            return false;
        }

        if (buffer_size <= NODE_SIZE || buffer == nullptr)
        {
            return false;
        }

        void* buffer_start_including_header = reinterpret_cast<void*>(reinterpret_cast<std::size_t>(buffer) - sizeof(*this)); // In case a caller is placing this object instance and the memory held by this instance sequentially

        if (!AlignmentChecks::is_address_page_allocation_granularity_aligned(buffer) && !AlignmentChecks::is_address_page_allocation_granularity_aligned(buffer_start_including_header))
        {
            return false; // You have to pass start of actual virtual memory pages which will be page size aligned
        }

        this->m_page_header.initialise();
        this->m_page_header.m_size_class = size_class;
        this->m_page_header.m_logical_page_start_address = reinterpret_cast<uint64_t>(buffer);
        this->m_page_header.m_logical_page_size = buffer_size;

        // Creating very first freelist node
        NodeType* head_node = static_cast<NodeType*>(buffer);
        head_node->set_block_size(buffer_size); // Including the memory used for NodeType itself,
                                                // so the max data that can be stored is is block_size - NODE_SIZE

        head_node->clear_next_pointer();

        insert(nullptr, head_node);

        #ifdef UNIT_TEST
        this->m_capacity = buffer_size;
        #endif

        return true;
    }

    ALIGN_CODE(AlignmentConstants::CACHE_LINE_SIZE) [[nodiscard]]
    void* allocate(const std::size_t size)
    {
        std::size_t allocation_size = size < NODE_SIZE ? NODE_SIZE : size;

        NodeType* affected_node = nullptr;
        NodeType* previous_node = nullptr;

        std::size_t padding{ 0 };

        find(allocation_size, previous_node, affected_node, LogicalPageAnySizeNode::MIN_ALIGNMENT_SIZE, padding);

        if (unlikely(affected_node == nullptr))
        {
            // Not enough memory
            return nullptr;
        }

        std::size_t total_size = allocation_size + padding + NODE_SIZE;
        const std::size_t excess_bytes = affected_node->get_block_size() - total_size;

        if (excess_bytes >= NODE_SIZE)
        {
            // We are returning extra bytes to the freelist as a new node
            NodeType* new_free_node = reinterpret_cast<NodeType*>(reinterpret_cast<std::size_t>(affected_node) + total_size /*padding + NODE_SIZE + allocation_size*/);
            new_free_node->set_block_size(excess_bytes);
            insert(affected_node, new_free_node);
        }
        else if (excess_bytes > 0 && excess_bytes < NODE_SIZE)
        {
            // Add extra bytes to the node we are about to release from the freelist
            // as it doesn't have enough space to hold a NodeType
            total_size += excess_bytes;
        }

        remove(previous_node, affected_node);

        // Calculate addresses based on the layout which is : <PADDING_BYTES><HEADER><PAYLOAD>
        const std::size_t header_address = reinterpret_cast<std::size_t>(affected_node) + padding;
        const std::size_t payload_address = header_address + NODE_SIZE;

        // Write the allocation header to the memory
        (reinterpret_cast<NodeType*>(header_address))->set_block_size(total_size);
        (reinterpret_cast<NodeType*>(header_address))->set_padding_size(padding);

        this->m_page_header.m_used_size += total_size;

        auto ret = reinterpret_cast<void*>(payload_address);

        #ifdef UNIT_TEST
        is_sane();
        #endif

        return ret;
    }

    ALIGN_CODE(AlignmentConstants::CACHE_LINE_SIZE)
    void deallocate(void* ptr)
    {
        if( unlikely(this->owns_pointer(ptr) == false) )
        {
            return;
        }

        const NodeType* allocation_header{ reinterpret_cast<NodeType*>(reinterpret_cast<std::size_t>(ptr) - NODE_SIZE) };
        const std::size_t current_address = reinterpret_cast<std::size_t>(ptr);
        std::size_t header_address = current_address - NODE_SIZE;
        std::size_t padding_size = allocation_header->get_padding_size();

        NodeType* free_node = reinterpret_cast<NodeType*>(header_address-padding_size);
        free_node->set_block_size(allocation_header->get_block_size());
        free_node->clear_next_pointer();

        NodeType* head = reinterpret_cast<NodeType*>(this->m_page_header.m_head);
        NodeType* iterator = head;
        NodeType* iterator_previous = nullptr;

        if (likely(head))
        {
            if (head->get_next_pointer() == 0)
            {
                insert(head, free_node);
            }
            else
            {
                bool inserted = false;
                while (iterator != nullptr)
                {
                    if (ptr < iterator)
                    {
                        insert(iterator_previous, free_node);
                        inserted = true;
                        break;
                    }
                    iterator_previous = iterator;
                    iterator = reinterpret_cast<NodeType*>(iterator->get_next_pointer());
                }

                // Inserting as the last element
                if (inserted == false)
                {
                    insert(iterator_previous, free_node);
                }
            }
        }
        else
        {
            insert(iterator_previous, free_node);
        }

        this->m_page_header.m_used_size -= free_node->get_block_size();

        if constexpr (coalesce_policy == CoalescePolicy::COALESCE)
        {
            // Merge them
            coalesce(iterator_previous, free_node);
        }

        #ifdef UNIT_TEST
        is_sane();
        #endif

    }

    static constexpr bool supports_any_size() { return true; }

    std::size_t get_usable_size(void* ptr)
    {
        const NodeType* allocation_header{ reinterpret_cast<NodeType*>(reinterpret_cast<std::size_t>(ptr) - NODE_SIZE) };
        return allocation_header->get_block_size() - NODE_SIZE;
    }

    #ifdef UNIT_TEST
    const std::string get_type_name() const { return "LogicalPageAnySize"; }

    double get_metadata_percentage_for_size_class(std::size_t size_class)
    {
        std::size_t single_data_metadata_pair_size = this->m_page_header.m_size_class + NODE_SIZE;
        std::size_t num_payloads = static_cast<std::size_t>(this->m_capacity / single_data_metadata_pair_size);
        std::size_t total_metadata_bytes = num_payloads * NODE_SIZE;
        return static_cast<double>(total_metadata_bytes) * 100 / this->m_capacity;
    }

    std::size_t get_node_count()
    {
        std::size_t count{ 0 };
        NodeType* iterator = reinterpret_cast<NodeType*>(this->m_page_header.m_head);
        while (iterator != nullptr)
        {
            count++;
            iterator = reinterpret_cast<NodeType*>(iterator->get_next_pointer());
        }
        return count;
    }

    bool is_sane()
    {
        NodeType* iterator = reinterpret_cast<NodeType*>(this->m_page_header.m_head);
        std::size_t total_available_memory{ 0 };

        while (iterator != nullptr)
        {
            total_available_memory += iterator->get_block_size();
            iterator = reinterpret_cast<NodeType*>(iterator->get_next_pointer());
        }

        auto computed_total_size = total_available_memory + this->m_page_header.m_used_size;

        bool is_sane = this->m_page_header.m_logical_page_size == computed_total_size;

        if (!is_sane)
        {
            fprintf(stderr, "CORRUPT FREELIST !!!");
        }

        return is_sane;
    }

    const std::string get_debug_info()
    {
        std::string output;
        NodeType* iterator = reinterpret_cast<NodeType*>(this->m_page_header.m_head);

        std::size_t counter{ 0 };
        std::size_t total_available_memory{ 0 };

        while (iterator != nullptr)
        {
            counter++;
            total_available_memory += iterator->get_block_size();
            output += "Node " + std::to_string(counter) + " : block size = " + std::to_string(iterator->get_block_size()) + "\n";
            iterator = reinterpret_cast<NodeType*>(iterator->get_next_pointer());
        }

        output += "USED SIZE = " + std::to_string(this->m_page_header.m_used_size) + "\n";
        output += "REMAINING SIZE = " + std::to_string(total_available_memory) + "\n";
        output += "USED + REMAINING SIZE = " + std::to_string(total_available_memory  + this->m_page_header.m_used_size) + "\n";
        output += "TOTAL = " + std::to_string(total_available_memory + this->m_page_header.m_used_size) + "\n";

        return output;
    }

    #endif

private:

    static inline constexpr std::size_t NODE_SIZE = sizeof(NodeType);

    void coalesce(NodeType* __restrict previous_node, NodeType* __restrict free_node)
    {
        if (reinterpret_cast<void*>(free_node->get_next_pointer()) != nullptr &&
            reinterpret_cast<std::size_t>(free_node) + free_node->get_block_size() == free_node->get_next_pointer())
        {
            // Free node and its next node are adjacent , adding the next node to free node and deleting the next node
            free_node->set_block_size(free_node->get_block_size() + reinterpret_cast<NodeType*>(free_node->get_next_pointer())->get_block_size());
            remove(free_node, reinterpret_cast<NodeType*>(free_node->get_next_pointer()));

            this->m_page_header.m_last_used_node = 0;
        }

        if (previous_node != nullptr &&
            reinterpret_cast<std::size_t>(previous_node) + previous_node->get_block_size() == reinterpret_cast<std::size_t>(free_node))
        {
            // Free node and its previous node are also adjacent , adding free node to the previous one and removing free node
            previous_node->set_block_size(previous_node->get_block_size() + free_node->get_block_size());
            remove(previous_node, free_node);

            this->m_page_header.m_last_used_node = 0;
        }
    }

    FORCE_INLINE void find(const std::size_t size, NodeType*& __restrict previous_node, NodeType*& __restrict found_node, const std::size_t alignment, std::size_t& padding_bytes)
    {
        NodeType* iterator = reinterpret_cast<NodeType*>(this->m_page_header.m_head);
        find_internal(iterator, nullptr, size, previous_node, found_node, alignment, padding_bytes);
    }

    FORCE_INLINE void find_internal(NodeType*& __restrict search_start_node, NodeType* __restrict search_end_node, const std::size_t size, NodeType*& __restrict previous_node, NodeType*& __restrict found_node, const std::size_t alignment, std::size_t& padding_bytes)
    {
        NodeType* iterator = search_start_node;
        NodeType* iterator_previous = nullptr;

        while (iterator && iterator != search_end_node)
        {
            padding_bytes = calculate_padding_needed<NODE_SIZE>(reinterpret_cast<std::size_t>(iterator), alignment);
            const std::size_t required_space = size + NODE_SIZE + padding_bytes;

            if (iterator->get_block_size() >= required_space)
            {
                previous_node = iterator_previous;
                found_node = iterator;
                break;
            }

            NodeType* next = reinterpret_cast<NodeType*>(iterator->get_next_pointer());

            if (next)
            {
                iterator_previous = iterator;
            }

            iterator = next;
        }
    }

    void insert(NodeType* __restrict previous_node, NodeType* __restrict new_node)
    {
        NodeType* head = reinterpret_cast<NodeType*>(this->m_page_header.m_head);

        if (previous_node == nullptr)
        {
            // Will be inserted as head

            if (head != nullptr)
            {
                new_node->set_next_pointer(this->m_page_header.m_head);
            }
            else
            {
                new_node->clear_next_pointer();
            }

            this->m_page_header.m_head = reinterpret_cast<uint64_t>(new_node);
        }
        else
        {
            if ( reinterpret_cast<void *>(previous_node->get_next_pointer()) == nullptr)
            {
                // Becomes last element
                previous_node->set_next_pointer(reinterpret_cast<uint64_t>(new_node));
                new_node->clear_next_pointer();
            }
            else
            {
                // Gets into the middle
                new_node->set_next_pointer(previous_node->get_next_pointer());
                previous_node->set_next_pointer(reinterpret_cast<uint64_t>(new_node));
            }
        }
    }

    void remove(NodeType* __restrict previous_node, NodeType* __restrict delete_node)
    {
        if (previous_node == nullptr)
        {
            if ( reinterpret_cast<NodeType*>(delete_node->get_next_pointer()) == nullptr)
            {
                // Deleting head
                this->m_page_header.m_head = 0;
            }
            else
            {
                // Second node becomes head
                this->m_page_header.m_head = delete_node->get_next_pointer();
            }
        }
        else
        {
            // Deleting from middle
            previous_node->set_next_pointer(delete_node->get_next_pointer());
        }
    }

#ifdef UNIT_TEST
public:
#else
private:
#endif

    template<std::size_t node_size>
    static const std::size_t calculate_padding_needed(const uint64_t base_address, const std::size_t alignment)
    {
        std::size_t header_end_address = base_address + node_size;
        std::size_t remainder = header_end_address % alignment;

        std::size_t extra_padding = 0;

        if (remainder != 0)
        {
            extra_padding = (alignment - remainder);
        }

        return extra_padding;
    }
};

#endif
/*
    UNBOUNDED THREAD SAFE QUEUE FOR STORING 64 BIT POINTERS
    STORES THEM IN A DOUBLY LINKED LIST OF 64KB "POINTER PAGE"S
*/
#ifndef __DEALLOCATION_QUEUE__
#define __DEALLOCATION_QUEUE__

// FIRST 16 BYTES OF EACH PAGE IS "next" and "prev" PTRS
// THE REST WILL BE USED TO STORE POINTERS
PACKED
(
    struct PointerPage
    {
        static constexpr std::size_t POINTER_CAPACITY = 8190; // 8190=65536-(2*8)/8
        PointerPage* m_next = nullptr;
        PointerPage* m_prev = nullptr;
        uint64_t m_pointers[POINTER_CAPACITY] = { 0 };
    }
);

template <typename AllocatorType>
class DeallocationQueue : public Lockable<LockPolicy::USERSPACE_LOCK>
{
    public:

        DeallocationQueue() = default;

        ~DeallocationQueue()
        {
            this->enter_concurrent_context();
            auto iter = m_head;
            while (iter)
            {
                auto next = iter->m_next;
                AllocatorType::deallocate(iter, sizeof(PointerPage));
                iter = next;
            }
            this->leave_concurrent_context();
        }

        [[nodiscard]] bool create(std::size_t initial_pointer_page_count, void* external_buffer = nullptr)
        {
            if (!initial_pointer_page_count) { return false; }

            PointerPage* buffer = nullptr;

            if (external_buffer == nullptr)
            {
                buffer = reinterpret_cast<PointerPage*>(AllocatorType::allocate(initial_pointer_page_count * sizeof(PointerPage)));

                if (buffer == nullptr)
                {
                    return false;
                }
            }
            else
            {
                buffer = reinterpret_cast<PointerPage*>(external_buffer);
            }

            builtin_memset(reinterpret_cast<void*>(buffer), 0, initial_pointer_page_count * sizeof(PointerPage));

            m_head = buffer;
            m_head->m_next = nullptr;
            m_head->m_prev = nullptr;

            auto iter = m_head;

            for (std::size_t i = 0; i < initial_pointer_page_count; i++)
            {
                if (i + 1 < initial_pointer_page_count)
                {
                    auto* next = &(buffer[i + 1]);
                    iter->m_next = next;
                    next->m_prev = iter;
                }
                else
                {
                    iter->m_next = nullptr;
                }
            }

            m_active_page = m_head;

            return true;
        }

        FORCE_INLINE void push(void* pointer)
        {
            this->enter_concurrent_context();
            ////////////////////////////////////
            if (unlikely(m_active_page_used_count == PointerPage::POINTER_CAPACITY))
            {
                // We need to start using a new page

                if (m_active_page->m_next == nullptr)
                {
                    #ifdef ENABLE_PERF_TRACES // INSIDE ALLOCATION CALLSTACK SO CAN'T ALLOCATE MEMORY HENCE OUTPUT TO stderr
                    fprintf(stderr, "deallocation queue grow\n");
                    #endif

                    // We need to allocate a new page
                    auto new_page = reinterpret_cast<PointerPage*>(AllocatorType::allocate(sizeof(PointerPage)));
                    new_page->m_next = nullptr;

                    m_active_page->m_next = new_page;
                    new_page->m_prev = m_active_page;
                    m_active_page = new_page;
                }
                else
                {
                    m_active_page = m_active_page->m_next;
                }

                m_active_page_used_count = 0;
            }

            m_active_page->m_pointers[m_active_page_used_count] = reinterpret_cast<uint64_t>(pointer);
            m_active_page_used_count++;
            ////////////////////////////////////
            this->leave_concurrent_context();
        }

        FORCE_INLINE [[nodiscard]] void* pop()
        {
            void* ret = nullptr;
            this->enter_concurrent_context();
            ////////////////////////////////////
            if (m_active_page_used_count == 0)
            {
                if (m_head == m_active_page)
                {
                    this->leave_concurrent_context();
                    return nullptr;
                }
                else
                {
                    m_active_page = m_active_page->m_prev;
                    m_active_page_used_count = PointerPage::POINTER_CAPACITY;
                }
            }

            ret = reinterpret_cast<void*>(m_active_page->m_pointers[m_active_page_used_count - 1]);
            m_active_page_used_count--;

            ////////////////////////////////////
            this->leave_concurrent_context();
            return ret;
        }

        DeallocationQueue(const DeallocationQueue& other) = delete;
        DeallocationQueue& operator= (const DeallocationQueue& other) = delete;
        DeallocationQueue(DeallocationQueue&& other) = delete;
        DeallocationQueue& operator=(DeallocationQueue&& other) = delete;

    private:
        PointerPage* m_head = nullptr;
        PointerPage* m_active_page = nullptr;
        std::size_t m_active_page_used_count = 0;
};

#endif
/*
    - A SEGMENT IS A COLLECTION OF LOGICAL PAGES. IT MAKES IT EASIER TO MANAGE MULTIPLE LOGICAL_PAGES :

                1. AS FOR ALLOCATIONS, WHEN MEMORY CONSUMPTION IS HIGH , SEQUENTIAL SEARCH FROM STARTING PAGE CAN BE VERY SLOW FOR. IT UTILISES "NEXT FIT" SO SEARCHES START FROM THE LOGICAL PAGE WHICH WAS THE LAST ALLOCATOR

                2. WE CAN GIVE COMPLETELY FREE PAGES BACK TO SYSTEM IF WE WILL NOT FALL UNDER "MINIMUM_LOGICAL_PAGES" THRESHOLD . OTHERWISE MEMORY CONSUMPTION OF ALLOCATOR WILL NEVER GO DOWN
                  ( WE LIMIT LOGICAL PAGES TO VM PAGE SIZES MORE EASILY SO THAT OBJECT DON'T END UP IN MULTIPLE PAGES )

    - INITIALLY HAS CONTINOUS PAGES LIKE SPANS/PAGE RUNS. HOWEVER THAT CAN CHANGE IF IT IS AN UNBOUNDED SEGMENT

    - MINIMUM LOGICAL PAGE SIZE FOR LINUX IS 4KB/4096 ON LINUX AND 64KB/65536 ON WINDOWS. LOGICAL PAGE SIZE ALSO HAS TO BE A MULTIPLE OF VIRTUAL MEMORY PAGE ALLOCATION GRANULARITY.

    - IT WILL PLACE A LOGICAL PAGE HEADER TO INITIAL 64 BYTES OF EVERY LOGICAL PAGE.

    - METADATA USAGE : PAGE HEADERS IN METAMALLOC ARE 64 BYTES THEREFORE FOR METADATA, WE WILL USE 64 BYTES PER EACH LOGICAL PAGE.
      IF SELECTED PAGE SIZE 4096 BYTES , 64 BYTES IN 4096 IS 0.78%
*/
#ifndef __SEGMENT_H__
#define __SEGMENT_H__

enum class PageRecyclingPolicy
{
    IMMEDIATE,    // IF LOGICAL PAGE COUNT > PAGE RECYCLING THRESHOLD VALUE, RECYCLING DONE AT THE END OF DEALLOCATIONS.
    DEFERRED    // USER HAS TO CALL "recycle_free_logical_pages"
};

enum class ConcurrencyPolicy
{
                        // BOUNDEDNESS                    DESCRIPTION

    THREAD_LOCAL,        // Bounded , can't grow            Partial locking needed. Deallocates just push ptrs to a spinlock based q and they quit, as they can come from multiple threads.
                        //                                Allocs will come from only one thread and they do actual deallocation by consuming dealloc q.
    CENTRAL,            // Unbounded, can grow            Segment level locking needed
    SINGLE_THREAD        // Unbounded, can grow            No locks
};

struct SegmentCreationParameters
{
    std::size_t m_logical_page_size= 0;
    std::size_t m_logical_page_count = 0;
    std::size_t m_page_recycling_threshold = 0;
    std::size_t m_deallocation_queue_initial_capacity = 65536;
    uint32_t m_size_class = 0;                             // 0 means that that segment will hold arbitrary size. Otherwise it will be for only 1 sizeclass.
    double m_grow_coefficient = 0.0;                     // 0 means that we will be growing by allocating only required amount
};

#ifdef ENABLE_STATS
static inline constexpr std::size_t MAX_GROW_STAT_COUNT = 32;

struct SegmentStats
{
    std::size_t m_size_class = 0;
    std::size_t m_latest_logical_page_count = 0;
    std::size_t m_recycle_count =0;
    std::size_t m_grow_size_count = 0;
    std::array<std::size_t, MAX_GROW_STAT_COUNT> m_grow_sizes {0};
};
#endif

template <
            ConcurrencyPolicy concurrency_policy,
            typename LogicalPageType,
            typename ArenaType,
            PageRecyclingPolicy page_recycling_policy = PageRecyclingPolicy::IMMEDIATE,
            bool aligned_logical_page_addresses = false
        >
class Segment : public Lockable<LockPolicy::USERSPACE_LOCK>
{
    public:

        Segment()
        {
            static_assert(std::is_base_of<ArenaBase<ArenaType>, ArenaType>::value);
            m_logical_page_object_size = sizeof(LogicalPageType);
        }

        ~Segment()
        {
            destroy();
        }

        Segment(const Segment& other) = delete;
        Segment& operator= (const Segment& other) = delete;
        Segment(Segment&& other) = delete;
        Segment& operator=(Segment&& other) = delete;

        [[nodiscard]] bool create(char* external_buffer, ArenaType* arena_ptr, const SegmentCreationParameters& params)
        {
            if (params.m_size_class < 0 || params.m_logical_page_size <= 0 || MultipleUtilities::is_size_a_multiple_of_page_allocation_granularity(params.m_logical_page_size) == false
                || params.m_logical_page_count <= 0 || params.m_logical_page_size <= m_logical_page_object_size || !external_buffer || !arena_ptr)
            {
                return false;
            }

            if constexpr (concurrency_policy == ConcurrencyPolicy::THREAD_LOCAL)
            {
                if (m_deallocation_queue.create(params.m_deallocation_queue_initial_capacity/sizeof(PointerPage), ArenaType::MetadataAllocator::allocate(params.m_deallocation_queue_initial_capacity)) == false)
                {
                    return false;
                }

                m_deallocation_queue_processing_lock.initialise();
            }

            m_arena = arena_ptr;
            m_logical_page_size = params.m_logical_page_size;
            m_max_object_size = m_logical_page_size - sizeof(LogicalPageHeader);
            m_size_class = params.m_size_class;
            m_page_recycling_threshold = params.m_page_recycling_threshold;
            m_grow_coefficient = params.m_grow_coefficient;

            if (grow(external_buffer, params.m_logical_page_count) == nullptr)
            {
                return false;
            }

            if constexpr (concurrency_policy == ConcurrencyPolicy::THREAD_LOCAL)
            {
                // We are bounded , we want to know our buffer limit
                m_buffer_length = m_logical_page_size * params.m_logical_page_count;
            }

            return true;
        }

        // size=0 means that underlying logical page can hold only one size which is 'm_size_class' which is set by create
        ALIGN_CODE(AlignmentConstants::CACHE_LINE_SIZE) [[nodiscard]]
        void* allocate(std::size_t size = 0)
        {
            if constexpr (concurrency_policy == ConcurrencyPolicy::THREAD_LOCAL)
            {
                // THREAD LOCAL
                if constexpr (LogicalPageType::supports_any_size()==false) // Underyling type should be used for same size class
                {
                    void* pointer = process_deallocation_queue<true>(); // While pointers in the deallocation queue are deallocated ,one of them will be returned to the allocation requestor

                    if (pointer)
                    {
                        return pointer;
                    }
                    else
                    {
                        return allocate_internal(size);
                    }
                }
                else
                {
                    process_deallocation_queue<false>();
                    return allocate_internal(size);
                }
            }
            else if constexpr (concurrency_policy == ConcurrencyPolicy::CENTRAL)
            {
                // CENTRAL , we are locking the entire segment
                this->enter_concurrent_context();
                auto ret = allocate_internal(size);
                this->leave_concurrent_context();
                return ret;
            }
            else
            {
                // SINGLE THREADED, NO LOCKS NEEDED
                return allocate_internal(size);
            }
        }

        ALIGN_CODE(AlignmentConstants::CACHE_LINE_SIZE)
        void deallocate(void* ptr)
        {
            if( m_head == nullptr ) { return;}

            if constexpr (concurrency_policy == ConcurrencyPolicy::THREAD_LOCAL)
            {
                // THREAD LOCAL
                m_deallocation_queue.push(ptr); // Q is thread safe
            }
            else if constexpr(concurrency_policy == ConcurrencyPolicy::CENTRAL)
            {
                // CENTRAL , we are locking entire segment
                this->enter_concurrent_context();
                deallocate_internal(ptr);
                this->leave_concurrent_context();
            }
            else
            {
                // SINGLE THREADED , NO LOCKS NEEDED
                deallocate_internal(ptr);
            }
        }

        // MAY BE CALLED FROM HEAP DEALLOCATION METHODS. IN CASE OF THREAD LOCAL OR CENTRAL HEAP , THAT MEANS MULTIPLE THREADS
        ALIGN_CODE(AlignmentConstants::CACHE_LINE_SIZE)
        bool owns_pointer(void* ptr)
        {
            if constexpr (concurrency_policy == ConcurrencyPolicy::THREAD_LOCAL || concurrency_policy == ConcurrencyPolicy::CENTRAL)
            {
                this->enter_concurrent_context();
            }

            if constexpr (concurrency_policy == ConcurrencyPolicy::THREAD_LOCAL)
            {
                // BOUNDED BUFFER
                uint64_t address_in_question = reinterpret_cast<uint64_t>(ptr);

                if (address_in_question >= reinterpret_cast<uint64_t>(m_head) && address_in_question < (reinterpret_cast<uint64_t>(m_head) + m_buffer_length))
                {
                    if constexpr (concurrency_policy == ConcurrencyPolicy::THREAD_LOCAL || concurrency_policy == ConcurrencyPolicy::CENTRAL)
                    {
                        this->leave_concurrent_context();
                    }
                    return true;
                }
            }
            else
            {
                // UNBOUNDED BUFFER , WE NEED A SEARCH
                std::size_t address = reinterpret_cast<std::size_t>(ptr);

                LogicalPageType* iter = m_head;

                while (iter != nullptr)
                {
                    std::size_t start = reinterpret_cast<std::size_t>(iter);
                    std::size_t end = start + m_logical_page_size;

                    if (address >= start && address < end)
                    {
                        if constexpr (concurrency_policy == ConcurrencyPolicy::THREAD_LOCAL || concurrency_policy == ConcurrencyPolicy::CENTRAL)
                        {
                            this->leave_concurrent_context();
                        }
                        return true;
                    }

                    iter = reinterpret_cast<LogicalPageType*>(iter->get_next_logical_page());
                }

            }

            if constexpr (concurrency_policy == ConcurrencyPolicy::THREAD_LOCAL || concurrency_policy == ConcurrencyPolicy::CENTRAL)
            {
                this->leave_concurrent_context();
            }
            return false;
        }

        void recycle_free_logical_pages()
        {
            // Should be called only when using deferred recycling
            if constexpr (page_recycling_policy == PageRecyclingPolicy::IMMEDIATE)
            {
                assert(0 == 1);
            }

            this->enter_concurrent_context();
            ///////////////////////////////////////////////////////////////////
            auto num_logical_pages_to_recycle = m_logical_page_count - m_page_recycling_threshold;
            LogicalPageType* iter = m_head;
            LogicalPageType* iter_previous = nullptr;

            while (num_logical_pages_to_recycle)
            {
                if (iter->can_be_recycled())
                {
                    recycle_logical_page(iter); // This method will update iter_previous's next ptr
                    iter = reinterpret_cast<LogicalPageType*>(iter_previous->get_next_logical_page());
                    num_logical_pages_to_recycle--;
                }
                else
                {
                    iter_previous = iter;
                    iter = reinterpret_cast<LogicalPageType*>(iter->get_next_logical_page());
                }
            }

            ///////////////////////////////////////////////////////////////////
            this->leave_concurrent_context();
        }

        void transfer_logical_pages_from(Segment& from)
        {
            this->enter_concurrent_context();
            ///////////////////////////////////////////////////////////////////
            LogicalPageType* iter = from.m_head;

            while (iter)
            {
                LogicalPageType* iter_next = reinterpret_cast<LogicalPageType*>(iter->get_next_logical_page());

                add_logical_page(iter); // Will also update iter's next ptr
                from.remove_logical_page(iter);

                iter = iter_next;
            }
            ///////////////////////////////////////////////////////////////////
            this->leave_concurrent_context();
        }

        std::size_t get_usable_size(void* ptr)
        {
            if constexpr (LogicalPageType::supports_any_size() == false)
            {
                return m_size_class;
            }
            else
            {
                LogicalPageType* iter = m_head;
                std::size_t address = reinterpret_cast<std::size_t>(ptr);

                while (iter != nullptr)
                {
                    std::size_t start = reinterpret_cast<std::size_t>(iter);
                    std::size_t end = start + m_logical_page_size;

                    if (address >= start && address < end)
                    {
                        return iter->get_usable_size(ptr);
                    }

                    iter = reinterpret_cast<LogicalPageType*>(iter->get_next_logical_page());
                }

                return 0;
            }
        }

        // Constant time logical page look up method for finding logical pages if their start addresses are aligned to logical page size
        static LogicalPageType* get_logical_page_from_address(void* ptr, std::size_t logical_page_size)
        {
            uint64_t orig_ptr = reinterpret_cast<uint64_t>(ptr);
            // Masking below is equivalent of -> orig_ptr - ModuloUtilities::modulo(orig_ptr, logical_page_size);
            uint64_t target_page_address = orig_ptr & ~(logical_page_size - 1);
            LogicalPageType* target_logical_page = reinterpret_cast<LogicalPageType*>(target_page_address);
            return target_logical_page;
        }

        // Constant time size_class look up method for finding logical pages
        static uint32_t get_size_class_from_address(void* ptr, std::size_t logical_page_size)
        {
            LogicalPageType* target_logical_page = get_logical_page_from_address(ptr, logical_page_size);
            return target_logical_page->get_size_class();
        }

        void lock_pages()
        {
            this->enter_concurrent_context();
            ///////////////////////////////////////////////////////////////////
            LogicalPageType* iter = m_head;

            while (iter)
            {
                VirtualMemory::lock(reinterpret_cast<void*>(iter), m_logical_page_size);
                iter->mark_as_locked();
                iter = reinterpret_cast<LogicalPageType*>(iter->get_next_logical_page());
            }
            ///////////////////////////////////////////////////////////////////
            this->leave_concurrent_context();
        }

        void unlock_pages()
        {
            this->enter_concurrent_context();
            ///////////////////////////////////////////////////////////////////
            LogicalPageType* iter = m_head;

            while (iter)
            {
                VirtualMemory::unlock(reinterpret_cast<void*>(iter), m_logical_page_size);
                iter->mark_as_non_locked();
                iter = reinterpret_cast<LogicalPageType*>(iter->get_next_logical_page());
            }
            ///////////////////////////////////////////////////////////////////
            this->leave_concurrent_context();
        }

        #ifdef UNIT_TEST
        std::size_t get_logical_page_count() const { return m_logical_page_count; }
        #endif

        #ifdef ENABLE_STATS
        SegmentStats get_stats() { m_stats.m_latest_logical_page_count = m_logical_page_count; return m_stats; }
        #endif

    private:
        uint32_t m_size_class = 0;                    // if m_size_class is zero that means, underlying logical page can hold any size
        std::size_t m_logical_page_size = 0;            // It includes also m_logical_page_object_size
        std::size_t m_max_object_size = 0;
        std::size_t m_logical_page_object_size = 0;
        std::size_t m_logical_page_count = 0;
        std::size_t m_buffer_length = 0;  // Applies to only bounded segments
        LogicalPageType* m_head = nullptr;
        LogicalPageType* m_tail = nullptr;
        LogicalPageType* m_last_used = nullptr;
        std::size_t m_page_recycling_threshold = 0; // In auto page recycling mode, if a logical page is free after deallocation ,
                                                    // it will be given back to system if free l.page count is over that threshold
        double m_grow_coefficient = 1.0;            // Applies to unbounded segments
        DeallocationQueue<typename ArenaType::MetadataAllocator> m_deallocation_queue;
        ArenaType* m_arena = nullptr;
        UserspaceSpinlock<> m_deallocation_queue_processing_lock;

        #ifdef ENABLE_STATS
        SegmentStats m_stats;
        #endif

        // Returns first logical page ptr of the grow
        [[nodiscard]] LogicalPageType* grow(char* buffer, std::size_t logical_page_count)
        {
            LogicalPageType* first_new_logical_page = nullptr;
            LogicalPageType* previous_page = m_tail;
            LogicalPageType* iter_page = nullptr;

            auto create_new_logical_page = [&](char* logical_page_buffer) -> bool
            {
                iter_page = new(logical_page_buffer) LogicalPageType();

                bool success = iter_page->create(logical_page_buffer + m_logical_page_object_size, m_logical_page_size - m_logical_page_object_size, m_size_class);

                if (success == false)
                {
                    m_arena->release_to_system(buffer, m_logical_page_size);
                    return false;
                }

                iter_page->mark_as_used();

                m_logical_page_count++;

                return true;
            };
            /////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
            // FIRST PAGE
            if (unlikely(create_new_logical_page(buffer) == false))
            {
                return nullptr;
            }

            first_new_logical_page = iter_page;

            if (m_head == nullptr)
            {
                // The very first page
                m_head = iter_page;
                m_tail = iter_page;
            }
            else
            {
                previous_page->set_next_logical_page(iter_page);
                iter_page->set_previous_logical_page(previous_page);
            }

            previous_page = iter_page;

            /////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
            // REST OF THE PAGES
            for (std::size_t i = 1; i < logical_page_count; i++)
            {
                if (create_new_logical_page(buffer + (i * m_logical_page_size)) == false)
                {
                    return nullptr;
                }

                previous_page->set_next_logical_page(iter_page);
                iter_page->set_previous_logical_page(previous_page);
                previous_page = iter_page;
            }

            m_tail = iter_page;

            return first_new_logical_page;
        }

        void recycle_logical_page(LogicalPageType* affected)
        {
            remove_logical_page(affected);
            affected->~LogicalPageType();
            m_arena->release_to_system(affected, m_logical_page_size);
            #ifdef ENABLE_STATS
            m_stats.m_recycle_count++;
            #endif

        }

        void remove_logical_page(LogicalPageType* affected)
        {
            auto next = reinterpret_cast<LogicalPageType*>(affected->get_next_logical_page());
            auto previous = reinterpret_cast<LogicalPageType*>(affected->get_previous_logical_page());

            if (affected == m_last_used)
            {
                if (previous)
                {
                    m_last_used = previous;
                }
                else if (next)
                {
                    m_last_used = next;
                }
                else
                {
                    m_last_used = nullptr;
                }
            }

            if (previous == nullptr)
            {
                m_head = next;

                if (m_head == nullptr || m_head->get_next_logical_page() == 0)
                {
                    m_tail = m_head;
                }
            }
            else
            {
                previous->set_next_logical_page(next);

                if (m_tail == affected)
                {
                    m_tail = previous;
                }
            }

            if (next)
                next->set_previous_logical_page(previous);

            m_logical_page_count--;
        }

        void add_logical_page(LogicalPageType* logical_page)
        {
            if (m_tail)
            {
                m_tail->set_next_logical_page(logical_page);
                logical_page->set_previous_logical_page(m_tail);
            }
            else
            {
                m_head = logical_page;
                m_tail = logical_page;
            }

            logical_page->set_next_logical_page(nullptr);
            m_logical_page_count++;
        }

        void destroy()
        {
            if constexpr (concurrency_policy==ConcurrencyPolicy::THREAD_LOCAL)
            {
                process_deallocation_queue();
            }

            LogicalPageType* iter = m_head;
            LogicalPageType* next = nullptr;

            while (iter)
            {
                next = reinterpret_cast<LogicalPageType*>(iter->get_next_logical_page());
                /////////////////////////////////////////////////////////////////////////////

                #ifdef ENABLE_REPORT_LEAKS
                if (iter->get_used_size() != 0)
                {
                    // Using fprintf with stderr so we don't allocate memory
                    if(m_size_class!=0)
                        fprintf(stderr, "Potential memory leak : sizeclass=%zu count=%zu\n", static_cast<std::size_t>(m_size_class), static_cast<std::size_t>(iter->get_used_size() / m_size_class));
                    else
                        fprintf(stderr, "Potential memory leak : total size=%zu \n", static_cast<std::size_t>(iter->get_used_size()));
                }
                #endif

                // There still maybe unloaded dynamic libraries (shared objects or DLLs) loaded by the host process
                // Even worse there may be unloaded but memory-leaking dynamic libraries
                // If we destroy a segment which has not been deallocated fully, we will crash, therefore this check is needed
                // If it is not possible to destroy then OS will reclaim virtual memory pages as soon as the host process dies.
                if (iter->get_used_size() == 0)
                {
                    // Invoking dtor of logical page
                    iter->~LogicalPageType();
                    // Release pages back to system if we are managing the arena
                    m_arena->release_to_system(iter, m_logical_page_size);
                }

                /////////////////////////////////////////////////////////////////////////////
                iter = next;
            }

            m_head = nullptr;
            m_tail = nullptr;
        }

        // DEALLOCATES ALL POINTERS IN THE DEALLOCATION QUEUE
        // IF THE CALLER IS ALLOCATOR INITIAL POINTER WON'T BE DEALLOCATED BUT INSTEAD RETURNED THE CALLER
        // TO SERVE ALLOCATIONS AS FAST AS POSSIBLE
        template <bool is_caller_allocator=false>
        void* process_deallocation_queue()
        {
            void* ret = nullptr;

            while (true)
            {
                auto pointer = m_deallocation_queue.pop();

                if (pointer == nullptr)
                {
                    break;
                }

                if constexpr (is_caller_allocator)
                {
                    if (likely(ret != nullptr))
                    {
                        deallocate_internal(pointer);
                    }
                    else
                    {
                        ret = pointer;
                    }
                }
                else
                {
                    deallocate_internal(pointer);
                }
            }

            return ret;
        }

        [[nodiscard]] void* allocate_internal(std::size_t size)
        {

            if (unlikely(size > m_max_object_size))
            {
                return nullptr;
            }

            ///////////////////////////////////////////////////////////////////
            void* ret = nullptr;
            LogicalPageType* iter = m_last_used ? m_last_used : m_head;

            // Next-fit like , we start searching from where we left if possible
            while (iter)
            {
                ret = iter->allocate(size);

                if (ret != nullptr)
                {
                    m_last_used = iter;

                    return ret;
                }

                iter = reinterpret_cast<LogicalPageType*>(iter->get_next_logical_page());
            }
            ///////////////////////////////////////////////////////////////////
            // If we started the search from a non-head node,  then we need one more iteration
            if (m_last_used)
            {
                iter = m_head;

                while (iter != m_last_used)
                {
                    ret = iter->allocate(size);

                    if (ret != nullptr)
                    {
                        m_last_used = iter;

                        return ret;
                    }

                    iter = reinterpret_cast<LogicalPageType*>(iter->get_next_logical_page());
                }
            }
            ///////////////////////////////////////////////////////////////////
            // If we reached here , it means that we need to allocate more memory
            if constexpr (concurrency_policy == ConcurrencyPolicy::CENTRAL || concurrency_policy == ConcurrencyPolicy::SINGLE_THREAD) // Only unbounded concurrecy policies can grow
            {
                std::size_t new_logical_page_count = 0;
                std::size_t minimum_new_logical_page_count = 0;
                calculate_quantities(size, new_logical_page_count, minimum_new_logical_page_count);

                char* new_buffer = nullptr;
                new_buffer = static_cast<char*>(m_arena->allocate(m_logical_page_size * new_logical_page_count));

                if (new_buffer == nullptr && new_logical_page_count > minimum_new_logical_page_count)  // Meeting grow_coefficient is not possible so lower the new_logical_page_count
                {
                    new_buffer = static_cast<char*>(m_arena->allocate(m_logical_page_size * minimum_new_logical_page_count));
                }

                if (!new_buffer)
                {
                    return nullptr;
                }

                auto first_new_logical_page = grow(new_buffer, new_logical_page_count);

                #ifdef ENABLE_PERF_TRACES // INSIDE ALLOCATION CALLSTACK SO CAN'T ALLOCATE MEMORY HENCE OUTPUT TO stderr
                fprintf(stderr, "segment grow size=%zu  sizeclass=%u\n", size, m_size_class);
                #endif

                #ifdef ENABLE_STATS
                if(m_stats.m_grow_size_count<MAX_GROW_STAT_COUNT)
                {
                    m_stats.m_grow_sizes[m_stats.m_grow_size_count] = new_logical_page_count;
                    m_stats.m_grow_size_count++;
                }
                #endif

                if (first_new_logical_page)
                {
                    ret = first_new_logical_page->allocate(size);

                    if (ret != nullptr)
                    {
                        m_last_used = iter;

                        return ret;
                    }
                }

            }

            // OUT OF MEMORY !
            return nullptr;
        }

        void calculate_quantities(const std::size_t size, std::size_t& desired_new_logical_page_count, std::size_t& minimum_new_logical_page_count)
        {
            if constexpr (LogicalPageType::supports_any_size()) // That means size will be arbitrary therefore the size may not suffice
            {
                minimum_new_logical_page_count = SizeUtilities::get_required_page_count_for_allocation(m_logical_page_size, m_logical_page_object_size, size, 1);
            }
            else
            {
                minimum_new_logical_page_count = SizeUtilities::get_required_page_count_for_allocation(m_logical_page_size, m_logical_page_object_size, m_size_class, size / m_size_class);
            }

            if ( likely(m_grow_coefficient > 0))
            {
                desired_new_logical_page_count = static_cast<std::size_t>(m_logical_page_count * m_grow_coefficient);

                if (desired_new_logical_page_count < minimum_new_logical_page_count)
                {
                    desired_new_logical_page_count = minimum_new_logical_page_count;
                }
            }
            else
            {
                desired_new_logical_page_count = minimum_new_logical_page_count;
            }
        }

        void deallocate_internal(void* ptr)
        {
            if constexpr (aligned_logical_page_addresses == true)
            {
                deallocate_from_aligned_logical_page(ptr);
            }
            else
            {
                deallocate_by_search(ptr);
            }
        }

        // WARNING : WHEN CALLING THIS ONE YOU HAVE TO MAKE SURE THAT ALL LOGICAL PAGES ARE
        // PLACED AT m_logical_page_size ALIGNED ADDRESSES
        void deallocate_from_aligned_logical_page(void* ptr)
        {
            auto affected = get_logical_page_from_address(ptr, m_logical_page_size);

            affected->deallocate(ptr);

            if (affected->get_used_size() == 0)
            {
                affected->mark_as_non_used();

                if constexpr (page_recycling_policy == PageRecyclingPolicy::IMMEDIATE)
                {
                    if (m_logical_page_count > m_page_recycling_threshold)
                    {
                        recycle_logical_page(affected);
                    }
                }
            }
        }

        // MAKES A LINEAR SEARCH
        void deallocate_by_search(void* ptr)
        {
            std::size_t address = reinterpret_cast<std::size_t>(ptr);
            LogicalPageType* iter = m_head;

            while (iter != nullptr)
            {
                std::size_t start = reinterpret_cast<std::size_t>(iter);
                std::size_t end = start + m_logical_page_size;

                if (address >= start && address < end)
                {
                    iter->deallocate(ptr);

                    if (iter->get_used_size() == 0)
                    {
                        iter->mark_as_non_used();

                        if constexpr (page_recycling_policy == PageRecyclingPolicy::IMMEDIATE)
                        {
                            if (m_logical_page_count > m_page_recycling_threshold)
                            {
                                recycle_logical_page(iter);
                            }
                        }
                    }

                    return;
                }
                iter = reinterpret_cast<LogicalPageType*>(iter->get_next_logical_page());
            }
        }
};

#endif
//INTERFACE FOR HEAPS WITH ONE CONCRETE IMPLEMENTATION : HeapBase::allocate_aligned
#ifndef __HEAP_BASE_H__
#define __HEAP_BASE_H__

template <typename HeapImplementation, ConcurrencyPolicy concurrency_policy = ConcurrencyPolicy::SINGLE_THREAD>
class HeapBase
{
    public:
        HeapBase() = default;
        ~HeapBase() = default;
        HeapBase(const HeapBase& other) = delete;
        HeapBase& operator= (const HeapBase& other) = delete;
        HeapBase(HeapBase&& other) = delete;
        HeapBase& operator=(HeapBase&& other) = delete;

        [[nodiscard]] void* allocate(std::size_t size) { return static_cast<HeapImplementation*>(this)->allocate(size); }

        /*
            Alignment has to be a power of two

            Note that if alignment sizes are too large , than memory will be wasted in unncesserily big padding bytes.
            If that is the case , override this method in your CRTP derived heap class
            And call this base method only if alignment size is small. Otherwise redirect it to a bin that uses logical_page_any_size ( for ex big object)
            as also logical_page_any_size can handle alignments and it will also be minimising used padding bytes during its search in its freelist
        */
        ALIGN_CODE(AlignmentConstants::CACHE_LINE_SIZE) [[nodiscard]]
        void* allocate_aligned(std::size_t size, std::size_t alignment)
        {
            if (alignment <= MINIMUM_ALIGNMENT)
            {
                // Framework already provides minimum 16 bit alignment
                return allocate(size);
            }

            std::size_t adjusted_size = size + alignment; // Adding padding bytes
            /////////////////////////////////////////////
            auto ptr = allocate(adjusted_size);
            /////////////////////////////////////////////
            std::size_t offset = alignment - (ModuloUtilities::modulo_pow2((reinterpret_cast<std::uint64_t>(ptr)), alignment));
            void* ret = reinterpret_cast<void*>(reinterpret_cast<std::uint64_t>(ptr) + offset);
            return ret;
        }

        // Should be called only from bounded heaps as unbounded heaps may not have contigious memory
        // Only Central and SingleThread concurrency policies are unbounded
        // In other words owns_pointer is supposed to be called from only Thread Local or CPU Local heaps
        ALIGN_CODE(AlignmentConstants::CACHE_LINE_SIZE) [[nodiscard]]
        bool owns_pointer(void* ptr)
        {
            if constexpr (concurrency_policy != ConcurrencyPolicy::THREAD_LOCAL)
            {
                assert(0 == 1);
            }

            uint64_t address_in_question = reinterpret_cast<uint64_t>(ptr);

            if ( address_in_question >= m_buffer_address && address_in_question < m_buffer_address + m_buffer_length )
            {
                return true;
            }

            return false;
        }

    protected:
        uint64_t m_buffer_address = 0;
        std::size_t m_buffer_length = 0;
        static constexpr inline std::size_t MINIMUM_ALIGNMENT = 16;
};

#endif

/*
    - THE ALLOCATOR WILL HAVE A CENTRAL HEAP AND ALSO THREAD OR CPU LOCAL HEAPS.

    - ALLOCATIONS INITIALLY WILL BE FROM LOCAL ( EITHER THREAD LOCAL OR CPU LOCAL ) HEAPS. IF LOCAL HEAPS ARE EXHAUSTED , THEN CENTRAL HEAP WILL BE USED.

    - USES CONFIGURABLE METADATA ( DEFAULT 128KB ) TO STORE LOCAL HEAPS.

    - YOU HAVE TO MAKE SURE THAT METADATA SIZE WILL BE ABLE TO HANDLE NUMBER OF THREADS IN YOUR APPLICATION.

*/
#ifndef __SCALABLE_ALLOCATOR__H__
#define __SCALABLE_ALLOCATOR__H__

template <
            typename CentralHeapType,
            typename LocalHeapType,
            typename ArenaType = Arena<>
         >
class ScalableAllocator : public Lockable<LockPolicy::USERSPACE_LOCK>
{
public:

    // THIS CLASS IS INTENDED TO BE USED DIRECTLY IN MALLOC REPLACEMENTS
    // SINCE THIS ONE IS A TEMPLATE CLASS , WE HAVE TO ENSURE A SINGLE ONLY STATIC VARIABLE INITIALISATION
    static ScalableAllocator& get_instance()
    {
        static ScalableAllocator instance;
        return instance;
    }

    [[nodiscard]] bool create(const typename CentralHeapType::HeapCreationParams& params_central, const typename LocalHeapType::HeapCreationParams& params_local, std::size_t arena_capacity, std::size_t arena_page_alignment = 65536, std::size_t metadata_buffer_size = 131072)
    {
        if (arena_capacity <= 0 || arena_page_alignment <= 0 || metadata_buffer_size <= 0 || !MultipleUtilities::is_size_a_multiple_of_page_allocation_granularity(arena_page_alignment) || !MultipleUtilities::is_size_a_multiple_of_page_allocation_granularity(metadata_buffer_size))
        {
            return false;
        }

        if (m_objects_arena.create(arena_capacity, arena_page_alignment) == false)
        {
            return false;
        }

        m_metadata_buffer_size = metadata_buffer_size;
        m_metadata_buffer = reinterpret_cast<char*>(ArenaType::MetadataAllocator::allocate(m_metadata_buffer_size));

        if (m_metadata_buffer == nullptr)
        {
            return false;
        }

        if (m_central_heap.create(params_central, &m_objects_arena) == false)
        {
            return false;
        }

        if (ThreadLocalStorage::get_instance().create(ScalableAllocator::thread_specific_destructor) == false)
        {
            return false;
        }

        m_local_heap_creation_params = params_local;

        if (!create_heaps())
        {
            return false;
        }

        m_initialised_successfully.store(true);

        return true;
    }

    void set_thread_local_heap_cache_count(std::size_t count)
    {
        m_cached_thread_local_heap_count = count;
    }

    /////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
    ALIGN_CODE(AlignmentConstants::CACHE_LINE_SIZE) [[nodiscard]]
    void* allocate(const std::size_t size)
    {
        #ifndef ENABLE_DEFAULT_MALLOC
        void* ret{ nullptr };
        auto local_heap = get_thread_local_heap();

        if (local_heap != nullptr)
        {
            ret = local_heap->allocate(size);
        }

        if (ret == nullptr)
        {
            #ifdef ENABLE_STATS
            m_central_heap_hit_count++;
            #endif

            #ifdef ENABLE_PERF_TRACES // INSIDE ALLOCATION CALLSTACK SO CAN'T ALLOCATE MEMORY HENCE OUTPUT TO stderr
            m_central_heap_hit_count++;
            fprintf(stderr, "scalable allocator , central heap hit count=%zu\n", m_central_heap_hit_count);
            #endif

            //If the local one is exhausted , failover to the central one
            ret = m_central_heap.allocate(size);
        }

        return ret;
        #else
        return builtin_aligned_alloc(size, AlignmentConstants::MINIMUM_VECTORISATION_WIDTH);
        #endif

    }

    ALIGN_CODE(AlignmentConstants::CACHE_LINE_SIZE) [[nodiscard]]
    void* allocate_aligned(std::size_t size, std::size_t alignment)
    {
        #ifndef ENABLE_DEFAULT_MALLOC
        void* ret{ nullptr };
        auto local_heap = get_thread_local_heap();

        if (local_heap != nullptr)
        {
            ret = local_heap->allocate_aligned(size, alignment);
        }

        if (ret == nullptr)
        {
            #ifdef ENABLE_STATS
            m_central_heap_hit_count++;
            #endif

            #ifdef ENABLE_PERF_TRACES // INSIDE ALLOCATION CALLSTACK SO CAN'T ALLOCATE MEMORY HENCE OUTPUT TO stderr
            m_central_heap_hit_count++;
            fprintf(stderr, "scalable allocator , central heap hit count=%zu\n", m_central_heap_hit_count);
            #endif

            //If the local one is exhausted , failover to the central one
            ret = m_central_heap.allocate_aligned(size, alignment);
        }

        return ret;
        #else
        return builtin_aligned_alloc(size, alignment);
        #endif

    }

    ALIGN_CODE(AlignmentConstants::CACHE_LINE_SIZE)
    void deallocate(void* ptr)
    {
        if (unlikely(ptr == nullptr))
        {
            return;
        }
        #ifndef ENABLE_DEFAULT_MALLOC
        // LINEAR SEARCH HOWEVER owns_pointer CHECK IS FAST IN BOUNDED LOCAL HEAPS
        // THEY DON'T DO ANOTHER INTERNAL LINEAR SEARCH THROUGH FREELISTS
        // SO THERE IS NO NESTED LINEAR SEARCHES BUT JUST ONE
        for (std::size_t i = 0; i < m_active_local_heap_count; i++)
        {
            LocalHeapType* local_heap = reinterpret_cast<LocalHeapType*>(m_metadata_buffer + (i * sizeof(LocalHeapType)));

            if (local_heap->owns_pointer(ptr))
            {
                local_heap->deallocate(ptr);
                return;
            }
        }
        // If we are here, ptr belongs to the central heap
        m_central_heap.deallocate(ptr);
        #else
        builtin_aligned_free(ptr);
        #endif

    }

    std::size_t get_usable_size(void* ptr)
    {
        if (ptr == nullptr) return 0;

        for (std::size_t i = 0; i < m_active_local_heap_count; i++)
        {
            LocalHeapType* local_heap = reinterpret_cast<LocalHeapType*>(m_metadata_buffer + (i * sizeof(LocalHeapType)));

            if (local_heap->owns_pointer(ptr))
            {
                return local_heap->get_usable_size(ptr);
            }
        }
        // If we are here, ptr belongs to the central heap
        return m_central_heap.get_usable_size(ptr);
    }

    CentralHeapType* get_central_heap() { return &m_central_heap; }

    #ifdef UNIT_TEST
    std::size_t get_observed_unique_thread_count() const { return m_observed_unique_thread_count; }
    #endif

    #ifdef ENABLE_STATS
    bool save_stats_to_file(const std::string_view file_name)
    {
        std::ofstream outfile(file_name.data());

        if (outfile.is_open())
        {
            outfile << "Central heap hit count = " << m_central_heap_hit_count << "\n";
            outfile << "Created thread local heap count = " << m_used_thread_local_heap_count << "\n\n";

            auto arena_stats = m_objects_arena.get_stats();
            outfile << "Virtual memory latest usage = " << SizeUtilities::get_human_readible_size(arena_stats.m_latest_used_size) << "\n";
            outfile << "Virtual memory allocation count = " << arena_stats.m_vm_allocation_count << "\n";

            for(std::size_t i=0; i< arena_stats.m_vm_allocation_count; i++)
            {
                outfile << "\tVirtual memory allocation size = " << SizeUtilities::get_human_readible_size(arena_stats.m_vm_allocation_sizes[i]) << "\n";
            }

            auto process_heap_stats = [&] (LocalHeapType* heap, const char* name)
            {
                auto heap_segments_stats = heap->get_stats();
                auto heap_segment_count = heap_segments_stats.size();

                outfile << "--------------------------------------------------------------\n";
                outfile << name << "\n\n";

                for (std::size_t i = 0; i < heap_segment_count; i++)
                {
                    outfile << "Heap segment sizeclass = " << heap_segments_stats[i].m_size_class << "\n";
                    outfile << "\t\tLatest logical page count = " << heap_segments_stats[i].m_latest_logical_page_count << "\n";
                    outfile << "\t\tRecycle count = " << heap_segments_stats[i].m_recycle_count << "\n";
                    outfile << "\t\tGrow count = " << heap_segments_stats[i].m_grow_size_count << "\n";

                    for (std::size_t j = 0; j < heap_segments_stats[i].m_grow_size_count; j++)
                    {
                        outfile << "\t\t\t\tGrow size = " << heap_segments_stats[i].m_grow_sizes[j] << "\n";
                    }
                }

                outfile << "--------------------------------------------------------------\n\n";
            };

            process_heap_stats(reinterpret_cast<LocalHeapType*>(&m_central_heap), "CENTRAL HEAP");

            std::size_t local_heap_count = m_active_local_heap_count;

            if(m_cached_thread_local_heap_count > local_heap_count )
            {
                local_heap_count = m_cached_thread_local_heap_count;
            }

            for (std::size_t i = 0; i < local_heap_count; i++)
            {
                LocalHeapType* local_heap = reinterpret_cast<LocalHeapType*>(m_metadata_buffer + (i * sizeof(LocalHeapType)));
                process_heap_stats(local_heap, "LOCAL HEAP");
            }

            outfile.close();
            return true;
        }
        return false;
    }
    #endif

    ///////////////////////////////////////////////////////////////////////////////////////////////////////////
    // WRAPPER METHODS FOR MALLOC REPLACEMENT/INTEGRATION
    [[nodiscard]] void* operator_new(std::size_t size)
    {
        void* ret = allocate(size);

        if( unlikely(ret==nullptr) )
        {
            handle_operator_new_failure();
        }

        return ret;
    }

    [[nodiscard]] void* operator_new_aligned(std::size_t size, std::size_t alignment)
    {
        void* ret = allocate_aligned(size, alignment);

        if( unlikely(ret==nullptr) )
        {
            handle_operator_new_failure();
        }

        return ret;
    }

    void handle_operator_new_failure()
    {
        std::new_handler handler;

        this->enter_concurrent_context();
        ///////////////////////////////////////
        handler = std::get_new_handler();
        ///////////////////////////////////////
        this->leave_concurrent_context();

        if(handler != nullptr)
        {
            handler();
        }
        else
        {
            throw std::bad_alloc();
        }
    }

    [[nodiscard]] void* allocate_and_zero_memory(std::size_t num, std::size_t size)
    {
        auto total_size = num * size;
        void* ret = allocate(total_size);

        if (ret != nullptr)
        {
            builtin_memset(ret, 0, total_size);
        }

        return ret;
    }

    [[nodiscard]] void* reallocate(void* ptr, std::size_t size)
    {
        if (ptr == nullptr)
            return  allocate(size);

        if (size == 0)
        {
            deallocate(ptr);
            return nullptr;
        }

        void* new_ptr = allocate(size);

        if (new_ptr != nullptr)
        {
            std::size_t old_size = get_usable_size(ptr);
            std::size_t copy_size = (old_size < size) ? old_size : size;
            builtin_memcpy(new_ptr, ptr, copy_size);
            deallocate(ptr);
        }

        return new_ptr;
    }

    [[nodiscard]] void* aligned_reallocate(void* ptr, std::size_t size, std::size_t alignment)
    {
        if (ptr == nullptr)
            return  allocate_aligned(size, alignment);

        if (size == 0)
        {
            deallocate(ptr);
            return nullptr;
        }

        void* new_ptr = allocate_aligned(size, alignment);

        if (new_ptr != nullptr)
        {
            std::size_t old_size = get_usable_size(ptr);
            std::size_t copy_size = (old_size < size) ? old_size : size;
            builtin_memcpy(new_ptr, ptr, copy_size);
            deallocate(ptr);
        }

        return new_ptr;
    }

    [[nodiscard]]void* reallocate_and_zero_memory(void *ptr, std::size_t num, std::size_t size)
    {
        auto total_size = num*size;
        auto ret = reallocate(ptr, total_size);

        if(ret != nullptr)
        {
            builtin_memset(ret, 0, total_size);
        }

        return ret;
    }
    ///////////////////////////////////////////////////////////////////////////////////////////////////////////

private:
    CentralHeapType m_central_heap;
    ArenaType m_objects_arena;
    char* m_metadata_buffer = nullptr;
    std::size_t m_metadata_buffer_size = 131072;       // Default 128KB
    std::size_t m_active_local_heap_count = 0;
    std::size_t m_max_thread_local_heap_count = 0;    // Used for only thread local heaps
    std::size_t m_cached_thread_local_heap_count = 0; // Used for only thread local heaps , its number of available passive heaps

    typename LocalHeapType::HeapCreationParams m_local_heap_creation_params;

    static inline std::atomic<bool> m_initialised_successfully = false;
    static inline std::atomic<bool> m_shutdown_started = false;

    #ifdef UNIT_TEST
    std::size_t m_observed_unique_thread_count = 0;
    #endif

    #ifdef ENABLE_STATS
    std::size_t m_central_heap_hit_count = 0;
    std::size_t m_used_thread_local_heap_count =0 ;
    #endif

    #ifdef ENABLE_PERF_TRACES
    std::size_t m_central_heap_hit_count = 0;
    #endif

    ScalableAllocator()
    {
        static_assert(std::is_base_of<HeapBase<CentralHeapType, ConcurrencyPolicy::CENTRAL>, CentralHeapType>::value);
        static_assert(std::is_base_of<HeapBase<LocalHeapType, ConcurrencyPolicy::THREAD_LOCAL>, LocalHeapType>::value || std::is_base_of<HeapBase<LocalHeapType, ConcurrencyPolicy::SINGLE_THREAD>, LocalHeapType>::value);
        static_assert(std::is_base_of<ArenaBase<ArenaType>, ArenaType>::value);
    }

    ~ScalableAllocator()
    {
        #ifdef ENABLE_STATS
        save_stats_to_file("metamalloc_stats.txt");
        #endif

        if(m_initialised_successfully.load() == true )
        {
            // We call it here it in case not called earlier and there are still running threads which are not destructed , no need to move logical pages between heaps
            m_shutdown_started.store(true);

            destroy_heaps();

            ThreadLocalStorage::get_instance().destroy();
        }
    }

    ScalableAllocator(const ScalableAllocator& other) = delete;
    ScalableAllocator& operator= (const ScalableAllocator& other) = delete;
    ScalableAllocator(ScalableAllocator&& other) = delete;
    ScalableAllocator& operator=(ScalableAllocator&& other) = delete;

    static void thread_specific_destructor(void* arg)
    {
        if( m_initialised_successfully.load() == true && m_shutdown_started.load() == false )
        {
            auto thread_local_heap = reinterpret_cast<LocalHeapType*>(arg);

            if(thread_local_heap) // Thread local arg is not supposed to be nullptr by OS specs but just to be safe
            {
                auto central_heap = get_instance().get_central_heap();
                central_heap->transfer_logical_pages_from(reinterpret_cast<CentralHeapType*>(thread_local_heap));
            }
        }
    }

    std::size_t get_created_heap_count()
    {
        auto heap_count = m_cached_thread_local_heap_count > m_active_local_heap_count ? m_cached_thread_local_heap_count : m_active_local_heap_count;
        return heap_count;
    }

    void destroy_heaps()
    {
        if (m_metadata_buffer)
        {
            auto heap_count = get_created_heap_count();

            for (std::size_t i = 0; i < heap_count; i++)
            {
                LocalHeapType* local_heap = reinterpret_cast<LocalHeapType*>(m_metadata_buffer + (i * sizeof(LocalHeapType)));
                local_heap->~LocalHeapType();
            }
        }
    }

    LocalHeapType* get_thread_local_heap()
    {
        return get_thread_local_heap_internal();
    }

    FORCE_INLINE LocalHeapType* get_thread_local_heap_internal()
    {
        auto thread_local_heap = reinterpret_cast<LocalHeapType*>(ThreadLocalStorage::get_instance().get());

        if (thread_local_heap == nullptr)
        {
            // LOCKING HERE WILL HAPPEN ONLY ONCE FOR EACH THREAD , AT THEIR START
            // AS THERE ARE SHARED VARIABLES FOR THREAD-LOCAL HEAP CREATION
            this->enter_concurrent_context();

            #ifdef UNIT_TEST
            m_observed_unique_thread_count++;
            #endif

            #ifdef ENABLE_STATS
            m_used_thread_local_heap_count++;
            #endif

            ///////////////////////////////////////////////////////////////////////////////////////////////////////////
            if (m_active_local_heap_count + 1 >= m_max_thread_local_heap_count)
            {
                // If we are here , it means that metadata buffer size is not sufficient to handle all threads of the application
                this->leave_concurrent_context();
                return nullptr;
            }

            if (m_active_local_heap_count >= m_cached_thread_local_heap_count)
            {
                thread_local_heap = create_local_heap(m_active_local_heap_count);
            }
            else
            {
                thread_local_heap = reinterpret_cast<LocalHeapType*>(m_metadata_buffer + (m_active_local_heap_count * sizeof(LocalHeapType)));
            }

            m_active_local_heap_count++;
            ThreadLocalStorage::get_instance().set(thread_local_heap);
            ///////////////////////////////////////////////////////////////////////////////////////////////////////////
            this->leave_concurrent_context();
        }

        return thread_local_heap;
    }

    bool create_heaps()
    {
        m_max_thread_local_heap_count = m_metadata_buffer_size / sizeof(LocalHeapType);

        if (m_max_thread_local_heap_count == 0)
        {
            return false;
        }

        if (m_max_thread_local_heap_count < m_cached_thread_local_heap_count)
        {
            m_cached_thread_local_heap_count = m_max_thread_local_heap_count;
        }

        for (std::size_t i{ 0 }; i < m_cached_thread_local_heap_count; i++)
        {
            auto local_heap = create_local_heap(i);
            if (!local_heap) return false;
        }

        return true;
    }

    LocalHeapType* create_local_heap(std::size_t metadata_buffer_index)
    {
        LocalHeapType* local_heap = new(m_metadata_buffer + (metadata_buffer_index * sizeof(LocalHeapType))) LocalHeapType();    // Placement new , does not invoke memory allocation

        if (local_heap->create(m_local_heap_creation_params, &m_objects_arena) == false)
        {
            return nullptr;
        }

        return local_heap;
    }
};

#endif

}
#endif